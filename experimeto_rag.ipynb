{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d39d63fd-63ac-49ae-873a-8dd62c6c96bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ RECUPERA√á√ÉO COMPLETA DO SISTEMA RAG\n",
      "============================================================\n",
      "1Ô∏è‚É£ Verificando bibliotecas...\n",
      "‚úÖ Todas as bibliotecas importadas com sucesso\n",
      "\n",
      "2Ô∏è‚É£ Carregando modelo de embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo carregado\n",
      "\n",
      "3Ô∏è‚É£ Recarregando documentos da pasta data...\n",
      "   ‚úÖ background.txt: 2079 chars\n",
      "   ‚úÖ coddigo.txt: 59433 chars\n",
      "   ‚úÖ 2506.11928v1.pdf: 71 p√°ginas\n",
      "   ‚úÖ Facial_Emotion_Recognition_(FER)_Advances_and_Applications_(2022-2025).pdf: 5 p√°ginas\n",
      "‚úÖ 4 documentos carregados\n",
      "\n",
      "4Ô∏è‚É£ Criando chunks...\n",
      "‚úÖ 325 chunks criados\n",
      "\n",
      "5Ô∏è‚É£ Criando banco vetorial...\n",
      "   Gerando embeddings...\n",
      "   Indexando no banco...\n",
      "‚úÖ 325 chunks indexados no banco vetorial\n",
      "\n",
      "============================================================\n",
      "üéâ SISTEMA RAG COMPLETAMENTE RESTAURADO!\n",
      "   üìö Documentos: 4\n",
      "   üß© Chunks: 325\n",
      "   üíæ Banco vetorial: 325 itens\n",
      "   üß† Modelo embedding: all-MiniLM-L6-v2\n",
      "\n",
      "üß™ Teste r√°pido de busca:\n",
      "‚úÖ Busca funcionando perfeitamente!\n",
      "\n",
      "üöÄ Agora voc√™ pode fazer buscas sem√¢nticas!\n"
     ]
    }
   ],
   "source": [
    "# C√âLULA DE RECUPERA√á√ÉO COMPLETA - Recria tudo do zero\n",
    "print(\"üîÑ RECUPERA√á√ÉO COMPLETA DO SISTEMA RAG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ========== PASSO 1: VERIFICAR BIBLIOTECAS ==========\n",
    "print(\"1Ô∏è‚É£ Verificando bibliotecas...\")\n",
    "try:\n",
    "    import sys\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from pathlib import Path\n",
    "    print(\"‚úÖ Todas as bibliotecas importadas com sucesso\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erro de importa√ß√£o: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ========== PASSO 2: CARREGAR MODELO DE EMBEDDING ==========\n",
    "print(\"\\n2Ô∏è‚É£ Carregando modelo de embedding...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úÖ Modelo carregado\")\n",
    "\n",
    "# ========== PASSO 3: RECARREGAR DOCUMENTOS ==========\n",
    "print(\"\\n3Ô∏è‚É£ Recarregando documentos da pasta data...\")\n",
    "data_path = Path(\"data\")\n",
    "documentos_reais = []\n",
    "metadados_docs = []\n",
    "\n",
    "if data_path.exists():\n",
    "    # Carregar TXTs\n",
    "    for arquivo_txt in data_path.glob(\"*.txt\"):\n",
    "        try:\n",
    "            loader = TextLoader(str(arquivo_txt), encoding='utf-8')\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                documentos_reais.append(doc.page_content)\n",
    "                metadados_docs.append({\n",
    "                    'source': arquivo_txt.name,\n",
    "                    'type': 'TXT',\n",
    "                    'size': len(doc.page_content)\n",
    "                })\n",
    "            print(f\"   ‚úÖ {arquivo_txt.name}: {len(docs[0].page_content)} chars\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Erro em {arquivo_txt.name}: {e}\")\n",
    "    \n",
    "    # Carregar PDFs\n",
    "    for arquivo_pdf in data_path.glob(\"*.pdf\"):\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(arquivo_pdf))\n",
    "            docs = loader.load()\n",
    "            texto_completo = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "            documentos_reais.append(texto_completo)\n",
    "            metadados_docs.append({\n",
    "                'source': arquivo_pdf.name,\n",
    "                'type': 'PDF',\n",
    "                'pages': len(docs),\n",
    "                'size': len(texto_completo)\n",
    "            })\n",
    "            print(f\"   ‚úÖ {arquivo_pdf.name}: {len(docs)} p√°ginas\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Erro em {arquivo_pdf.name}: {e}\")\n",
    "\n",
    "print(f\"‚úÖ {len(documentos_reais)} documentos carregados\")\n",
    "\n",
    "# ========== PASSO 4: CRIAR CHUNKS ==========\n",
    "print(\"\\n4Ô∏è‚É£ Criando chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks_reais = []\n",
    "for i, (doc_text, metadata) in enumerate(zip(documentos_reais, metadados_docs)):\n",
    "    doc_chunks = text_splitter.split_text(doc_text)\n",
    "    for j, chunk in enumerate(doc_chunks):\n",
    "        chunk_info = {\n",
    "            'id': f'doc_{i}_chunk_{j}',\n",
    "            'text': chunk,\n",
    "            'source_file': metadata['source'],\n",
    "            'source_type': metadata['type'],\n",
    "            'chunk_number': j,\n",
    "            'char_count': len(chunk)\n",
    "        }\n",
    "        chunks_reais.append(chunk_info)\n",
    "\n",
    "print(f\"‚úÖ {len(chunks_reais)} chunks criados\")\n",
    "\n",
    "# ========== PASSO 5: CRIAR BANCO VETORIAL ==========\n",
    "print(\"\\n5Ô∏è‚É£ Criando banco vetorial...\")\n",
    "client = chromadb.Client(Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    allow_reset=True\n",
    "))\n",
    "\n",
    "# Limpar collection anterior\n",
    "try:\n",
    "    client.delete_collection(\"meus_documentos_rag\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = client.create_collection(\"meus_documentos_rag\")\n",
    "\n",
    "# Preparar dados\n",
    "textos_chunks = [chunk['text'] for chunk in chunks_reais]\n",
    "ids_chunks = [f\"chunk_{i}\" for i in range(len(chunks_reais))]\n",
    "\n",
    "# Gerar embeddings\n",
    "print(\"   Gerando embeddings...\")\n",
    "embeddings_chunks = embedding_model.encode(textos_chunks)\n",
    "\n",
    "# Indexar\n",
    "print(\"   Indexando no banco...\")\n",
    "collection.add(\n",
    "    embeddings=embeddings_chunks.tolist(),\n",
    "    documents=textos_chunks,\n",
    "    ids=ids_chunks\n",
    ")\n",
    "\n",
    "total_items = collection.count()\n",
    "print(f\"‚úÖ {total_items} chunks indexados no banco vetorial\")\n",
    "\n",
    "# ========== VERIFICA√á√ÉO FINAL ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ SISTEMA RAG COMPLETAMENTE RESTAURADO!\")\n",
    "print(f\"   üìö Documentos: {len(documentos_reais)}\")\n",
    "print(f\"   üß© Chunks: {len(chunks_reais)}\")\n",
    "print(f\"   üíæ Banco vetorial: {total_items} itens\")\n",
    "print(f\"   üß† Modelo embedding: all-MiniLM-L6-v2\")\n",
    "\n",
    "# Teste r√°pido\n",
    "print(\"\\nüß™ Teste r√°pido de busca:\")\n",
    "resultado_teste = collection.query(\n",
    "    query_texts=[\"intelig√™ncia artificial\"],\n",
    "    n_results=1\n",
    ")\n",
    "print(\"‚úÖ Busca funcionando perfeitamente!\")\n",
    "\n",
    "print(\"\\nüöÄ Agora voc√™ pode fazer buscas sem√¢nticas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0639a3bd-068f-4596-969e-c8fb986bc834",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phyton: 3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:47:33) [MSC v.1943 64 bit (AMD64)]\n",
      "Verificando se as bibliotecas est√£o disponiveis...\n",
      "‚úÖ LangChain: 0.3.26\n",
      "‚úÖ ChromaDB: 1.0.15\n",
      "‚úÖ Sentence Transformers: 5.0.0\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 1: Verifica√ß√£o basica e importa√ß√µes \n",
    "import sys\n",
    "print(f\"Phyton: {sys.version}\")\n",
    "\n",
    "# Verifica se estamos no ambiente correto\n",
    "print(\"Verificando se as bibliotecas est√£o disponiveis...\")\n",
    "\n",
    "try:\n",
    "    import langchain\n",
    "    print(f\"‚úÖ LangChain: {langchain.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå LangChain n√£o encontrado\")\n",
    "\n",
    "try:\n",
    "    import chromadb\n",
    "    print(f\"‚úÖ ChromaDB: {chromadb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå ChromaDB n√£o encontrado\")\n",
    "\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    print(f\"‚úÖ Sentence Transformers: {sentence_transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Sentence Transformers n√£o encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f21ce73-ad3d-473f-8e88-8343db692213",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando modelo de embedding...\n",
      "‚úÖ Modelo Carregado!\n",
      "\n",
      "Vamos analisar 6 frases:\n",
      "1. 'O gato subiu no telhado'\n",
      "2. 'Um felino escalou o teto de casa'\n",
      "3. 'Hoje est√° chovendo muito'\n",
      "4. 'Tempestade ocorreu com intensidade'\n",
      "5. 'O cachorro late no quintal'\n",
      "6. 'O canino ladra mas n√£o morde'\n",
      "\n",
      "Gerando embeddings...\n",
      "‚úÖ Shape dos embeddings: (6, 384)\n",
      "‚úÖ Cada texto virou um vetor de 384 dimens√µes\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 2: Experimento com embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Carregar um modelo pequeno para entender embeddings\n",
    "print(\"Carregando modelo de embedding...\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"‚úÖ Modelo Carregado!\")\n",
    "\n",
    "# Teste b√°sico com frases similares e diferentes\n",
    "textos = [\n",
    "    \"O gato subiu no telhado\",\n",
    "    \"Um felino escalou o teto de casa\",\n",
    "    \"Hoje est√° chovendo muito\",\n",
    "    \"Tempestade ocorreu com intensidade\",\n",
    "    \"O cachorro late no quintal\",\n",
    "    \"O canino ladra mas n√£o morde\"\n",
    "]\n",
    "\n",
    "print(f\"\\nVamos analisar {len(textos)} frases:\")\n",
    "for i, texto in enumerate(textos):\n",
    "    print(f\"{i+1}. '{texto}'\")\n",
    "\n",
    "# Gerar embeddings\n",
    "print(\"\\nGerando embeddings...\")\n",
    "embeddings = embedding_model.encode(textos)\n",
    "print(f\"‚úÖ Shape dos embeddings: {embeddings.shape}\")\n",
    "print(f\"‚úÖ Cada texto virou um vetor de {embeddings.shape[1]} dimens√µes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7349c2c2-671c-49fa-afb9-99dca8778622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analisando a similaridade entre os nossos 6 textos:\n",
      "\n",
      "1. 'O gato subiu no telhado'\n",
      "2. 'Um felino escalou o teto de casa'\n",
      "3. 'Hoje est√° chovendo muito'\n",
      "4. 'Tempestade ocorreu com intensidade'\n",
      "5. 'O cachorro late no quintal'\n",
      "6. 'O canino ladra mas n√£o morde'\n",
      "\n",
      "============================================================\n",
      "üìä Matriz de similaridade:\n",
      "(1.0 = id√™ntico, 0.0 = neutro, -1.0 = oposto)\n",
      "\n",
      "Texto 1 vs Texto 2: 0.497\n",
      "  'O gato subiu no telhado...' ‚Üî 'Um felino escalou o teto de ca...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 1 vs Texto 3: 0.469\n",
      "  'O gato subiu no telhado...' ‚Üî 'Hoje est√° chovendo muito...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 1 vs Texto 4: 0.418\n",
      "  'O gato subiu no telhado...' ‚Üî 'Tempestade ocorreu com intensi...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 1 vs Texto 5: 0.548\n",
      "  'O gato subiu no telhado...' ‚Üî 'O cachorro late no quintal...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 1 vs Texto 6: 0.503\n",
      "  'O gato subiu no telhado...' ‚Üî 'O canino ladra mas n√£o morde...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 2 vs Texto 3: 0.456\n",
      "  'Um felino escalou o teto de ca...' ‚Üî 'Hoje est√° chovendo muito...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 2 vs Texto 4: 0.506\n",
      "  'Um felino escalou o teto de ca...' ‚Üî 'Tempestade ocorreu com intensi...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 2 vs Texto 5: 0.477\n",
      "  'Um felino escalou o teto de ca...' ‚Üî 'O cachorro late no quintal...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 2 vs Texto 6: 0.467\n",
      "  'Um felino escalou o teto de ca...' ‚Üî 'O canino ladra mas n√£o morde...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 3 vs Texto 4: 0.422\n",
      "  'Hoje est√° chovendo muito...' ‚Üî 'Tempestade ocorreu com intensi...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 3 vs Texto 5: 0.441\n",
      "  'Hoje est√° chovendo muito...' ‚Üî 'O cachorro late no quintal...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 3 vs Texto 6: 0.561\n",
      "  'Hoje est√° chovendo muito...' ‚Üî 'O canino ladra mas n√£o morde...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 4 vs Texto 5: 0.360\n",
      "  'Tempestade ocorreu com intensi...' ‚Üî 'O cachorro late no quintal...'\n",
      "  ‚ù§Ô∏è Pouco similares\n",
      "\n",
      "Texto 4 vs Texto 6: 0.412\n",
      "  'Tempestade ocorreu com intensi...' ‚Üî 'O canino ladra mas n√£o morde...'\n",
      "  üíõ Moderadamente similares\n",
      "\n",
      "Texto 5 vs Texto 6: 0.415\n",
      "  'O cachorro late no quintal...' ‚Üî 'O canino ladra mas n√£o morde...'\n",
      "  üíõ Moderadamente similares\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 3: Calculando a similaridade entre textos\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"üîç Analisando a similaridade entre os nossos 6 textos:\")\n",
    "print()\n",
    "\n",
    "# Relembrar quais s√£o os textos\n",
    "for i, texto in enumerate(textos):\n",
    "    print(f\"{i+1}. '{texto}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Calcular similaridade entre todos os textos\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"üìä Matriz de similaridade:\")\n",
    "print(\"(1.0 = id√™ntico, 0.0 = neutro, -1.0 = oposto)\")\n",
    "print()\n",
    "\n",
    "# Mostarr compara√ß√£o de forma organizada\n",
    "for i, texto1 in enumerate(textos):\n",
    "    for j, texto2 in enumerate(textos):\n",
    "        if i < j: # evita repeti√ß√µes (s√≥ mostra a metade da matriz)\n",
    "            sim = similarities[i][j]\n",
    "            print(f\"Texto {i+1} vs Texto {j+1}: {sim:.3f}\")\n",
    "            print(f\"  '{texto1[:30]}...' ‚Üî '{texto2[:30]}...'\")\n",
    "        \n",
    "            # Interpreta√ß√£o humana\n",
    "            if sim > 0.7:\n",
    "                print(f\"  üíö Muito similares!\")\n",
    "            elif sim > 0.4:\n",
    "                print(f\"  üíõ Moderadamente similares\")\n",
    "            else:\n",
    "                print(f\"  ‚ù§Ô∏è Pouco similares\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d65892a-5d43-4c61-b7e6-ff4a382f7144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Base de Conhecimetno Criada!\n",
      "==================================================\n",
      "Documento 1:\n",
      "  üìÑ 50 palavras, 369 caracteres\n",
      "  üìù Tema: Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia...\n",
      "\n",
      "Documento 2:\n",
      "  üìÑ 42 palavras, 334 caracteres\n",
      "  üìù Tema: Machine Learning √© uma subcategoria da IA que perm...\n",
      "\n",
      "Documento 3:\n",
      "  üìÑ 42 palavras, 299 caracteres\n",
      "  üìù Tema: Python √© uma linguagem de programa√ß√£o de alto n√≠ve...\n",
      "\n",
      "Documento 4:\n",
      "  üìÑ 42 palavras, 329 caracteres\n",
      "  üìù Tema: LangChain √© um framework para desenvolver aplica√ß√µ...\n",
      "\n",
      "Documento 5:\n",
      "  üìÑ 41 palavras, 309 caracteres\n",
      "  üìù Tema: Bancos vetoriais s√£o sistemas de banco de dados es...\n",
      "\n",
      "‚úÖ Total: 5 documentos em nossa base\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 4: Criando nossa \"base de conhecimento\"\n",
    "documentos = [\n",
    "    \"\"\"\n",
    "    Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra \n",
    "    no desenvolvimento de sistemas capazes de realizar tarefas que normalmente \n",
    "    requerem intelig√™ncia humana. Isso inclui aprendizado, racioc√≠nio, \n",
    "    percep√ß√£o e tomada de decis√£o. A IA pode ser classificada em IA fraca \n",
    "    (sistemas espec√≠ficos) e IA forte (intelig√™ncia geral).\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Machine Learning √© uma subcategoria da IA que permite que os computadores \n",
    "    aprendam e melhorem automaticamente a partir da experi√™ncia, sem serem \n",
    "    explicitamente programados. Utiliza algoritmos estat√≠sticos para \n",
    "    identificar padr√µes em dados. Os principais tipos s√£o: supervisionado, \n",
    "    n√£o supervisionado e por refor√ßo.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "     Python √© uma linguagem de programa√ß√£o de alto n√≠vel, interpretada e \n",
    "    de prop√≥sito geral. √â amplamente utilizada em ci√™ncia de dados, \n",
    "    desenvolvimento web, automa√ß√£o e intelig√™ncia artificial devido \n",
    "    √† sua sintaxe simples e rica biblioteca de pacotes como NumPy, \n",
    "    Pandas e TensorFlow.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    LangChain √© um framework para desenvolver aplica√ß√µes com modelos de \n",
    "    linguagem. Ele fornece ferramentas para conectar LLMs com fontes de \n",
    "    dados externas, criar pipelines de processamento e construir \n",
    "    aplica√ß√µes inteligentes como chatbots e sistemas de pergunta-resposta.\n",
    "    LangChain facilita a implementa√ß√£o de RAG.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Bancos vetoriais s√£o sistemas de banco de dados especializados em \n",
    "    armazenar e buscar vetores de alta dimensionalidade. Eles s√£o \n",
    "    fundamentais para aplica√ß√µes de IA que utilizam embeddings, como \n",
    "    sistemas de recomenda√ß√£o, busca sem√¢ntica e RAG. Exemplos incluem \n",
    "    ChromaDB, Pinecone e Weaviate.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(\"üìö Base de Conhecimetno Criada!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, doc in enumerate(documentos):\n",
    "    palavras = len(doc.split())\n",
    "    caracteres = len(doc.strip())\n",
    "    print(f\"Documento {i+1}:\")\n",
    "    print(f\"  üìÑ {palavras} palavras, {caracteres} caracteres\")\n",
    "    print(f\"  üìù Tema: {doc.strip()[:50]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"‚úÖ Total: {len(documentos)} documentos em nossa base\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bcc7210-6500-46b0-8dff-853acf6a40cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî™ Aprendendo sobre Chunking\n",
      "========================================\n",
      "‚öôÔ∏è Configura√ß√£o do Text Splitter:\n",
      "   üìè Tamanho m√°ximo do chunk: 200 caracteres\n",
      "   üîó Sobreposi√ß√£o: 50 caracteres\n",
      "   ‚úÇÔ∏è Separadores: ['\\n\\n', '\\n', '. ', ' ', '']\n",
      "\n",
      "üìÑ Documento 1 ‚Üí 3 chunk(s)\n",
      " Chunk 3: 55 chars ‚Üí '(sistemas espec√≠ficos) e IA forte (intelig√™ncia geral)....'\n",
      "üìÑ Documento 2 ‚Üí 2 chunk(s)\n",
      " Chunk 2: 179 chars ‚Üí 'explicitamente programados. Utiliza algoritmos estat√≠sticos ...'\n",
      "üìÑ Documento 3 ‚Üí 2 chunk(s)\n",
      " Chunk 2: 157 chars ‚Üí 'desenvolvimento web, automa√ß√£o e intelig√™ncia artificial dev...'\n",
      "üìÑ Documento 4 ‚Üí 2 chunk(s)\n",
      " Chunk 2: 183 chars ‚Üí 'dados externas, criar pipelines de processamento e construir...'\n",
      "üìÑ Documento 5 ‚Üí 2 chunk(s)\n",
      " Chunk 2: 171 chars ‚Üí 'fundamentais para aplica√ß√µes de IA que utilizam embeddings, ...'\n",
      "\n",
      "==================================================\n",
      "üéØ Resultado Final:\n",
      "   üìö 5 documentos originais\n",
      "   üß© 5 chunks criados\n",
      "   üìä M√©dia de 1.0 chunks por documento\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 5: Entendendo Chunking (Divis√£o de Texto)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"üî™ Aprendendo sobre Chunking\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Configurar o divisor de texto\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200, # Tamanho m√°ximo de cada peda√ßo de texto (em caracteres)\n",
    "    chunk_overlap=50, # Sobreposi√ß√£o entre peda√ßos\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # Como dividir (ordem de prefer√™ncia)\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è Configura√ß√£o do Text Splitter:\")\n",
    "print(f\"   üìè Tamanho m√°ximo do chunk: {text_splitter._chunk_size} caracteres\")\n",
    "print(f\"   üîó Sobreposi√ß√£o: {text_splitter._chunk_overlap} caracteres\")\n",
    "print(f\"   ‚úÇÔ∏è Separadores: {text_splitter._separators}\")\n",
    "print()\n",
    "\n",
    "# Dividir nossos docuemntos\n",
    "chunks = []\n",
    "chunk_counter = 0\n",
    "\n",
    "for i, doc in enumerate(documentos):\n",
    "    doc_limpo = doc.strip()\n",
    "    doc_chunks = text_splitter.split_text(doc_limpo)\n",
    "\n",
    "    print(f\"üìÑ Documento {i+1} ‚Üí {len(doc_chunks)} chunk(s)\")\n",
    "    \n",
    "    for j, chunk in enumerate(doc_chunks):\n",
    "         chunk_info = {\n",
    "        'id': f\"chunk_{chunk_counter}\",\n",
    "        'text': chunk,\n",
    "        'source': f\"documento_{i+1}\",\n",
    "        'chunk_num': j,\n",
    "        'chars': len(chunk)\n",
    "    }\n",
    "    chunks.append(chunk_info)\n",
    "    \n",
    "    print(f\" Chunk {j+1}: {len(chunk)} chars ‚Üí '{chunk[:60]}...'\")\n",
    "    chunk_counter += 1\n",
    "print()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"üéØ Resultado Final:\")\n",
    "print(f\"   üìö {len(documentos)} documentos originais\")\n",
    "print(f\"   üß© {len(chunks)} chunks criados\")\n",
    "print(f\"   üìä M√©dia de {len(chunks)/len(documentos):.1f} chunks por documento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0a70a21-9852-4bd2-8c4f-e328acec078a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Arquivos encontrados na pasta 'data':\n",
      "==================================================\n",
      "üìÑ PDF: 2506.11928v1.pdf (712.0 KB)\n",
      "üìù TXT: background.txt (2.1 KB)\n",
      "üìù TXT: coddigo.txt (60.7 KB)\n",
      "üìÑ PDF: Facial_Emotion_Recognition_(FER)_Advances_and_Applications_(2022-2025).pdf (78.3 KB)\n",
      "‚ùì Outro: imagem (8).png (38.0 KB)\n",
      "\n",
      "üìä Resumo:\n",
      "   üìÑ 2 arquivo(s) PDF\n",
      "   üìù 2 arquivo(s) TXT\n",
      "   ‚ùì 1 outro(s) arquivo(s)\n",
      "\n",
      "‚úÖ Vamos usar esses arquivos no nosso RAG!\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 6: Explorando arquivos reais na pasta data\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Vrificar pasta data\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "if data_path.exists():\n",
    "    print(\"üìÅ Arquivos encontrados na pasta 'data':\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    arquivo_pdf = []\n",
    "    arquivo_txt = []\n",
    "    outros_arquivos = []\n",
    "\n",
    "    for arquivo in data_path.iterdir():\n",
    "        if arquivo.is_file():\n",
    "            tamanho_kb = arquivo.stat().st_size / 1024\n",
    "\n",
    "            if arquivo.suffix.lower() == '.pdf':\n",
    "                arquivo_pdf.append(arquivo)\n",
    "                print(f\"üìÑ PDF: {arquivo.name} ({tamanho_kb:.1f} KB)\")\n",
    "            elif arquivo.suffix.lower() == '.txt':\n",
    "                arquivo_txt.append(arquivo)\n",
    "                print(f\"üìù TXT: {arquivo.name} ({tamanho_kb:.1f} KB)\")\n",
    "            else:\n",
    "                outros_arquivos.append(arquivo)\n",
    "                print(f\"‚ùì Outro: {arquivo.name} ({tamanho_kb:.1f} KB)\")\n",
    "                \n",
    "    print(f\"\\nüìä Resumo:\")\n",
    "    print(f\"   üìÑ {len(arquivo_pdf)} arquivo(s) PDF\")\n",
    "    print(f\"   üìù {len(arquivo_txt)} arquivo(s) TXT\")\n",
    "    print(f\"   ‚ùì {len(outros_arquivos)} outro(s) arquivo(s)\")\n",
    "\n",
    "    if len(arquivo_pdf) + len(arquivo_txt) > 0:\n",
    "        print(\"\\n‚úÖ Vamos usar esses arquivos no nosso RAG!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Nenhum PDF ou TXT encontrado. Vamos usar documentos sint√©ticos.\")\n",
    "else:\n",
    "    print(\"‚ùå Pasta 'data' n√£o encontrada.\")\n",
    "    print(\"üí° Crie a pasta 'data' e coloque seus arquivos l√°, ou vamos continuar com documentos sint√©ticos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd781a81-1c9a-4d85-aa11-3c1c108a2081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Carregando documentos reais...\n",
      "========================================\n",
      "üìù Carregando arquivos TXT:\n",
      "   ‚ùå Erro ao carregar background.txt: 'list' object has no attribute 'name'\n",
      "   ‚ùå Erro ao carregar coddigo.txt: 'list' object has no attribute 'name'\n",
      "üìù Carregando arquivos PDF:\n",
      "   ‚ùå Erro ao carregar 2506.11928v1.pdf: 'list' object has no attribute 'name'\n",
      "   ‚ùå Erro ao carregar Facial_Emotion_Recognition_(FER)_Advances_and_Applications_(2022-2025).pdf: 'list' object has no attribute 'name'\n",
      "\n",
      "==================================================\n",
      "üéØ Documentos carregados com sucesso:\n",
      "   üìÑ Total: 4 documento(s)\n",
      "   1. background.txt (TXT) - 2079 chars\n",
      "   2. coddigo.txt (TXT) - 59433 chars\n",
      "\n",
      "‚úÖ Vamos usar seus 4 documento(s) real(is) no RAG!\n",
      "\n",
      "üìÑ Amostra do primeiro documento (background.txt):\n",
      "'j√° trabalhei com Llama inclusive tenho participa√ß√£o da primeira equipe de IA em uma competi√ß√£o no Kaggle, trabalho diariamente com OpenAI, Gemini e Claude da Anthopric.\n",
      "Ainda n√£o tive oportunidade de ...'\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 7: Carergando documentos reais\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "print(\"üìö Carregando documentos reais...\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "documentos_reais = []\n",
    "metadados_docs = []\n",
    "\n",
    "# Carregar arquivos TXT\n",
    "if arquivo_txt:\n",
    "    print(\"üìù Carregando arquivos TXT:\")\n",
    "    for arquivos_txt in arquivo_txt:\n",
    "        try:\n",
    "            loader = TextLoader(str(arquivos_txt), encoding='utf-8')\n",
    "            docs = loader.load()\n",
    "\n",
    "            for doc in docs:\n",
    "                documentos_reais.append(doc.page_content)\n",
    "                metadados_docs.append({\n",
    "                    'source': arquivos_txt.name,\n",
    "                    'type': 'TXT',\n",
    "                    'size': len(doc.page_content)\n",
    "                })\n",
    "                \n",
    "            print(f\"   ‚úÖ {arquivo_txt.name}: {len(docs[0].page_content)} caracteres\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro ao carregar {arquivos_txt.name}: {e}\")\n",
    "\n",
    "# Carregar arquivo PDF\n",
    "if arquivo_pdf:\n",
    "    print(\"üìù Carregando arquivos PDF:\")\n",
    "    for arquivos_pdf in arquivo_pdf:\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(arquivos_pdf))\n",
    "            docs = loader.load()\n",
    "\n",
    "            # Juntar todas as paginas do PDF\n",
    "            texto_completo = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "            documentos_reais.append(texto_completo)\n",
    "\n",
    "            metadados_docs.append({\n",
    "                'source': arquivo_pdf.name,\n",
    "                'type': 'PDF',\n",
    "                'pages': len(docs),\n",
    "                'size': len(texto_completo)\n",
    "            })\n",
    "\n",
    "            print(f\"   ‚úÖ {arquivo_pdf.name}: {len(docs)} p√°ginas, {len(texto_completo)} caracteres\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro ao carregar {arquivos_pdf.name}: {e}\")\n",
    "\n",
    "# Resultado final\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"üéØ Documentos carregados com sucesso:\")\n",
    "print(f\"   üìÑ Total: {len(documentos_reais)} documento(s)\")\n",
    "\n",
    "for i, metadata in enumerate(metadados_docs):\n",
    "    print(f\"   {i+1}. {metadata['source']} ({metadata['type']}) - {metadata['size']} chars\")\n",
    "\n",
    "# Se n√£o conseguiu carregar nenhum, usar sint√©ticos\n",
    "if not documentos_reais:\n",
    "    print(\"\\n‚ö†Ô∏è Nenhum documento foi carregado. Usando documentos sint√©ticos...\")\n",
    "    # (aqui poderia voltar aos documentos sint√©ticos se necess√°rio)\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Vamos usar seus {len(documentos_reais)} documento(s) real(is) no RAG!\")\n",
    "    \n",
    "    # Mostrar uma amostra do primeiro documento\n",
    "    print(f\"\\nüìÑ Amostra do primeiro documento ({metadados_docs[0]['source']}):\")\n",
    "    print(f\"'{documentos_reais[0][:200]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d1c6ccf-2f77-401d-a461-e99789b9680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî™ Aplicando Chunking aos Documentos Reais\n",
      "==================================================\n",
      "‚öôÔ∏è Configura√ß√£o do Chunking:\n",
      "   üìè Tamanho m√°ximo: 1000 caracteres\n",
      "   üîó Sobreposi√ß√£o: 200 caracteres\n",
      "\n",
      "üìÑ Processando: background.txt\n",
      "   üìä Documento original: 2,079 caracteres\n",
      "   üß© Resultado: 4 chunks\n",
      "   üìà Tamanho dos chunks: 354-997 chars (m√©dia: 568)\n",
      "   üìù Primeiro chunk: 'j√° trabalhei com Llama inclusive tenho participa√ß√£o da primeira equipe de IA em uma competi√ß√£o no Ka...'\n",
      "\n",
      "üìÑ Processando: coddigo.txt\n",
      "   üìä Documento original: 59,433 caracteres\n",
      "   üß© Resultado: 71 chunks\n",
      "   üìà Tamanho dos chunks: 355-998 chars (m√©dia: 894)\n",
      "   üìù Primeiro chunk: '// >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<...'\n",
      "\n",
      "============================================================\n",
      "üéØ Resumo Final do Chunking:\n",
      "   üìö 4 documentos processados\n",
      "   üß© 75 chunks criados no total\n",
      "   üìä M√©dia de 18.8 chunks por documento\n",
      "\n",
      "üìã Detalhes por arquivo:\n",
      "   üìÑ background.txt: 4 chunks (m√©dia 568 chars)\n",
      "   üìÑ coddigo.txt: 71 chunks (m√©dia 894 chars)\n",
      "\n",
      "üèÜ Documento com mais chunks: coddigo.txt (71 chunks)\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 8: Aplicando chunking aos documentos reais\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"üî™ Aplicando Chunking aos Documentos Reais\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Configurar splitter para documentos reais (chunks maiores)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, # Maior para documentos reais\n",
    "    chunk_overlap = 200, # Overlap mairo para preservar contexto\n",
    "    separators = [\"\\n\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è Configura√ß√£o do Chunking:\")\n",
    "print(f\"   üìè Tamanho m√°ximo: {text_splitter._chunk_size} caracteres\")\n",
    "print(f\"   üîó Sobreposi√ß√£o: {text_splitter._chunk_overlap} caracteres\")\n",
    "print()\n",
    "\n",
    "# Processar cada documento\n",
    "chunk_reais = []\n",
    "estatisticas = []\n",
    "\n",
    "for i, (doc_text, metadata) in enumerate(zip(documentos_reais, metadados_docs)):\n",
    "    print(f\"üìÑ Processando: {metadata['source']}\")\n",
    "    print(f\"   üìä Documento original: {len(doc_text):,} caracteres\")\n",
    "\n",
    "    # Aplicar chunking\n",
    "    doc_chunks = text_splitter.split_text(doc_text)\n",
    "\n",
    "    print(f\"   üß© Resultado: {len(doc_chunks)} chunks\")\n",
    "\n",
    "    # Armazenar chunks com metadados\n",
    "    for j, chunk in enumerate(doc_chunks):\n",
    "        chunk_info = {\n",
    "            'id': f'doc_{i}_chunk_{j}',\n",
    "            'text': chunk,\n",
    "            'source_file': metadata['source'],\n",
    "            'source_type': metadata['type'],\n",
    "            'chunk_number': j,\n",
    "            'total_chunks': len(doc_chunks),\n",
    "            'char_count': len(chunk)\n",
    "        }\n",
    "        chunk_reais.append(chunk_info)\n",
    "\n",
    "    # Estatisticas para documento\n",
    "    tamanhos_chunks = [len(chunk) for chunk in doc_chunks]\n",
    "    stats = {\n",
    "        'arquivo': metadata['source'],\n",
    "        'chars_original': len(doc_text),\n",
    "        'num_chunks': len(doc_chunks),\n",
    "        'chunk_min': min(tamanhos_chunks),\n",
    "        'chunk_max': max(tamanhos_chunks),\n",
    "        'chunk_medio': sum(tamanhos_chunks) / len(tamanhos_chunks)\n",
    "    }\n",
    "    estatisticas.append(stats)\n",
    "\n",
    "    print(f\"   üìà Tamanho dos chunks: {stats['chunk_min']}-{stats['chunk_max']} chars (m√©dia: {stats['chunk_medio']:.0f})\")\n",
    "\n",
    "    # Mostrar amostra do primeiro chunk\n",
    "    print(f\"   üìù Primeiro chunk: '{doc_chunks[0][:100]}...'\")\n",
    "    print()\n",
    "\n",
    "# Resumo final\n",
    "print(\"=\"*60)\n",
    "print(\"üéØ Resumo Final do Chunking:\")\n",
    "print(f\"   üìö {len(documentos_reais)} documentos processados\")\n",
    "print(f\"   üß© {len(chunk_reais)} chunks criados no total\")\n",
    "print(f\"   üìä M√©dia de {len(chunk_reais)/len(documentos_reais):.1f} chunks por documento\")\n",
    "\n",
    "print(\"\\nüìã Detalhes por arquivo:\")\n",
    "for stat in estatisticas:\n",
    "    print(f\"   üìÑ {stat['arquivo']}: {stat['num_chunks']} chunks (m√©dia {stat['chunk_medio']:.0f} chars)\")\n",
    "\n",
    "# Encontrar o documento com mais chunks\n",
    "doc_mais_chunks = max(estatisticas, key=lambda x: x['num_chunks'])\n",
    "print(f\"\\nüèÜ Documento com mais chunks: {doc_mais_chunks['arquivo']} ({doc_mais_chunks['num_chunks']} chunks)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "098faefd-c72c-4cc2-904c-08b7b417af7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Criando Banco Vetorial - Vers√£o Simplificada\n",
      "============================================================\n",
      "üîç Verificando dados antes do processamento:\n",
      "   üìö documentos_reais: 4 itens\n",
      "   üß© chunks_reais: 325 itens\n",
      "   üìÑ Estrutura do primeiro chunk:\n",
      "      id: <class 'str'> = doc_0_chunk_0...\n",
      "      text: <class 'str'> = j√° trabalhei com Llama inclusive tenho participa√ß√£...\n",
      "      source_file: <class 'str'> = background.txt...\n",
      "      source_type: <class 'str'> = TXT...\n",
      "      chunk_number: <class 'int'> = 0...\n",
      "      char_count: <class 'int'> = 969...\n",
      "‚úÖ Cole√ß√£o 'meus_documentos_rag' criada\n",
      "\n",
      "üìä Dados preparados:\n",
      "   üìù 325 textos v√°lidos\n",
      "   üè∑Ô∏è 325 IDs criados\n",
      "   üìã 325 metadados criados\n",
      "\n",
      "üß† Gerando embeddings...\n",
      "‚úÖ Embeddings gerados:\n",
      "   ‚è±Ô∏è Tempo: 13.01 segundos\n",
      "   üìä Shape: (325, 384)\n",
      "\n",
      "üíæ Adicionando ao banco vetorial...\n",
      "‚úÖ Sucesso! 325 chunks indexados\n",
      "\n",
      "üß™ Teste r√°pido de busca:\n",
      "   ‚úÖ Busca funcionando: 3 resultados encontrados\n",
      "\n",
      "üéâ Banco vetorial est√° pronto!\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 9: Banco vetorial simplificado (sem metadados complexos)\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import time\n",
    "\n",
    "print(\"üèóÔ∏è Criando Banco Vetorial - Vers√£o Simplificada\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verificar dados antes de processar\n",
    "print(\"üîç Verificando dados antes do processamento:\")\n",
    "print(f\"   üìö documentos_reais: {len(documentos_reais)} itens\")\n",
    "print(f\"   üß© chunks_reais: {len(chunks_reais)} itens\")\n",
    "\n",
    "# Mostrar estrutura do primeiro chunk\n",
    "if len(chunks_reais) > 0:\n",
    "    primeiro_chunk = chunks_reais[0]\n",
    "    print(f\"   üìÑ Estrutura do primeiro chunk:\")\n",
    "    for key, value in primeiro_chunk.items():\n",
    "        print(f\"      {key}: {type(value)} = {str(value)[:50]}...\")\n",
    "else:\n",
    "    print(\"   ‚ùå Nenhum chunk encontrado!\")\n",
    "    exit()\n",
    "\n",
    "# Configurar ChromaDB\n",
    "client = chromadb.Client(Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    allow_reset=True\n",
    "))\n",
    "\n",
    "# Limpar e criar cole√ß√£o\n",
    "collection_name = \"meus_documentos_rag\"\n",
    "try:\n",
    "    client.delete_collection(collection_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = client.create_collection(name=collection_name)\n",
    "print(f\"‚úÖ Cole√ß√£o '{collection_name}' criada\")\n",
    "\n",
    "# Extrair dados dos chunks\n",
    "textos_chunks = []\n",
    "ids_chunks = []\n",
    "metadados_simples = []\n",
    "\n",
    "for i, chunk in enumerate(chunks_reais):\n",
    "    # Garantir que temos texto\n",
    "    if 'text' in chunk and chunk['text'].strip():\n",
    "        textos_chunks.append(chunk['text'])\n",
    "        ids_chunks.append(f\"chunk_{i}\")  # ID simples\n",
    "        \n",
    "        # Metadados muito simples (apenas arquivo fonte)\n",
    "        if 'source_file' in chunk:\n",
    "            metadados_simples.append({'source': chunk['source_file']})\n",
    "        else:\n",
    "            metadados_simples.append({'source': 'unknown'})\n",
    "\n",
    "print(f\"\\nüìä Dados preparados:\")\n",
    "print(f\"   üìù {len(textos_chunks)} textos v√°lidos\")\n",
    "print(f\"   üè∑Ô∏è {len(ids_chunks)} IDs criados\") \n",
    "print(f\"   üìã {len(metadados_simples)} metadados criados\")\n",
    "\n",
    "# Verificar se temos dados v√°lidos\n",
    "if len(textos_chunks) == 0:\n",
    "    print(\"‚ùå Nenhum texto v√°lido encontrado!\")\n",
    "    exit()\n",
    "\n",
    "# Gerar embeddings\n",
    "print(f\"\\nüß† Gerando embeddings...\")\n",
    "start_time = time.time()\n",
    "embeddings_chunks = embedding_model.encode(textos_chunks)\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Embeddings gerados:\")\n",
    "print(f\"   ‚è±Ô∏è Tempo: {embedding_time:.2f} segundos\")\n",
    "print(f\"   üìä Shape: {embeddings_chunks.shape}\")\n",
    "\n",
    "# Adicionar ao ChromaDB\n",
    "print(f\"\\nüíæ Adicionando ao banco vetorial...\")\n",
    "try:\n",
    "    collection.add(\n",
    "        embeddings=embeddings_chunks.tolist(),\n",
    "        documents=textos_chunks,\n",
    "        metadatas=metadados_simples,\n",
    "        ids=ids_chunks\n",
    "    )\n",
    "    \n",
    "    total_items = collection.count()\n",
    "    print(f\"‚úÖ Sucesso! {total_items} chunks indexados\")\n",
    "    \n",
    "    # Testar uma busca r√°pida\n",
    "    print(f\"\\nüß™ Teste r√°pido de busca:\")\n",
    "    resultados_teste = collection.query(\n",
    "        query_texts=[\"intelig√™ncia artificial\"],\n",
    "        n_results=3\n",
    "    )\n",
    "    print(f\"   ‚úÖ Busca funcionando: {len(resultados_teste['documents'][0])} resultados encontrados\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao adicionar: {e}\")\n",
    "    print(\"üí° Vamos tentar sem metadados...\")\n",
    "    \n",
    "    # Tentar sem metadados\n",
    "    try:\n",
    "        collection.add(\n",
    "            embeddings=embeddings_chunks.tolist(),\n",
    "            documents=textos_chunks,\n",
    "            ids=ids_chunks\n",
    "            # Sem metadatas\n",
    "        )\n",
    "        \n",
    "        total_items = collection.count()\n",
    "        print(f\"‚úÖ Sucesso sem metadados! {total_items} chunks indexados\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Erro mesmo sem metadados: {e2}\")\n",
    "\n",
    "print(f\"\\nüéâ Banco vetorial est√° pronto!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7e19e5d-4df2-46f7-8b83-8ac938ab9e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Primeira Busca Sem√¢ntica Real\n",
      "==================================================\n",
      "‚úÖ Banco vetorial dispon√≠vel: 325 chunks\n",
      "üß™ TESTE 1: Experi√™ncia Profissional\n",
      "\n",
      "‚ùì Pergunta: 'experi√™ncia com intelig√™ncia artificial e machine learning'\n",
      "------------------------------------------------------------\n",
      "üîç Debug - Chaves do resultado: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\n",
      "üìÑ Resultado 1 (ID: chunk_323):\n",
      "   üéØ Similaridade: -0.184 (quanto maior, mais similar)\n",
      "   üìù Texto encontrado:\n",
      "      '‚Ä¢ Explainable AI (XAI) in FER: Enhancing the interpretability of FER models to \n",
      "understand how they arrive at their predictions, which is crucial for building trust and \n",
      "addressing ethical concerns.\n",
      "‚Ä¢ Edge AI for FER: Deploying FER models on edge devices for real-time, on-device \n",
      "processing, reducin...'\n",
      "\n",
      "üìÑ Resultado 2 (ID: chunk_90):\n",
      "   üéØ Similaridade: -0.347 (quanto maior, mais similar)\n",
      "   üìù Texto encontrado:\n",
      "      'the same rules as humans. This pipeline is detailed in Appendix A3.\n",
      "Difficulty tiers. To streamline navigation, we adopt a Codeforces-style rating heuristic for difficulty\n",
      "labels. The Elo difficulty of a problem means that a contestant of the corresponding rating can\n",
      "solve the problem with a 50% pro...'\n",
      "\n",
      "üìÑ Resultado 3 (ID: chunk_322):\n",
      "   üéØ Similaridade: -0.356 (quanto maior, mais similar)\n",
      "   üìù Texto encontrado:\n",
      "      'of models across diverse real-world datasets and the need for more comprehensive \n",
      "datasets that account for variations in head position and other complexities. Ethical \n",
      "considerations, including privacy, algorithmic bias, and potential misuse, are also critical \n",
      "areas that require ongoing attention ...'\n",
      "\n",
      "\n",
      "üß™ TESTE 2: Competi√ß√µes e Kaggle\n",
      "\n",
      "‚ùì Pergunta: 'participa√ß√£o em competi√ß√£o Kaggle'\n",
      "------------------------------------------------------------\n",
      "üîç Debug - Chaves do resultado: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\n",
      "üìÑ Resultado 1 (ID: chunk_2):\n",
      "   üéØ Similaridade: -0.103 (quanto maior, mais similar)\n",
      "   üìù Texto encontrado:\n",
      "      'No Tribunal de Contas do Estado do PR como estagi√°rio pude observar projetos que poderiam ser otimizados em processos adminsitrativos, onde pude participar do primeiro chatbot alimentado com as normas internas da institui√ß√£o. \n",
      "Na Federa√ß√£o do Estado do Parana pude participar na implementa√ß√£o do chat...'\n",
      "\n",
      "üìÑ Resultado 2 (ID: chunk_0):\n",
      "   üéØ Similaridade: -0.186 (quanto maior, mais similar)\n",
      "   üìù Texto encontrado:\n",
      "      'j√° trabalhei com Llama inclusive tenho participa√ß√£o da primeira equipe de IA em uma competi√ß√£o no Kaggle, trabalho diariamente com OpenAI, Gemini e Claude da Anthopric.\n",
      "Ainda n√£o tive oportunidade de trabalhar com LangChain porem estou trabalhando com automa√ß√£o com N8N.\n",
      "J√° trabalhei com python e fas...'\n",
      "\n",
      "üìÑ Resultado 3 (ID: chunk_26):\n",
      "   üéØ Similaridade: -0.197 (quanto maior, mais similar)\n",
      "   üìù Texto encontrado:\n",
      "      '// Responda com:\n",
      "// \"Claro! Um dos nossos especialistas pode te atender pelo WhatsApp. Clique abaixo para conversar com a gente direto e tirar todas as suas d√∫vidas. üëâ (https://wa.me/5541987249685?text=)\"\n",
      "\n",
      "// ---\n",
      "\n",
      "// üìå **Quando n√£o identificar claramente a inten√ß√£o**, responda com foco em:\n",
      "// * Info...'\n",
      "\n",
      "\n",
      "üß™ TESTE 3: C√≥digo e Programa√ß√£o\n",
      "\n",
      "‚ùì Pergunta: 'c√≥digo Python machine learning'\n",
      "------------------------------------------------------------\n",
      "üîç Debug - Chaves do resultado: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\n",
      "üìÑ Resultado 1 (ID: chunk_317):\n",
      "   üéØ Similaridade: -0.143 (quanto maior, mais similar)\n",
      "   üìù Texto encontrado:\n",
      "      'address speciÔ¨Åc challenges.\n",
      "Traditional datasets like CK+ (Extended Cohn-Kanade Dataset), JAFFE (Japanese Female \n",
      "Facial Expression), and FER-2013 remain important benchmarks. However, research \n",
      "indicates that models trained on these datasets may face limitations when dealing with \n",
      "variations in hea...'\n",
      "\n",
      "üìÑ Resultado 2 (ID: chunk_131):\n",
      "   üéØ Similaridade: -0.156 (quanto maior, mais similar)\n",
      "   üìù Texto encontrado:\n",
      "      'LiveCodeBench Pro\n",
      "[52] Maxim Shipko. New features: friends, tags and more, 2011. URL https://codeforces.com/\n",
      "blog/entry/1679.\n",
      "[53] Jiaxin Wen, Ruiqi Zhong, Pei Ke, Zhihong Shao, Hongning Wang, and Minlie Huang. Learning\n",
      "task decomposition to assist humans in competitive programming. In Proceedings o...'\n",
      "\n",
      "üìÑ Resultado 3 (ID: chunk_76):\n",
      "   üéØ Similaridade: -0.208 (quanto maior, mais similar)\n",
      "   üìù Texto encontrado:\n",
      "      'LiveCodeBench Pro\n",
      "Abstract. Recent reports claim that large language models (LLMs) now outperform elite humans\n",
      "in competitive programming. Drawing on knowledge from a group of medalists in international\n",
      "algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and\n",
      "whe...'\n",
      "\n",
      "\n",
      "üß™ TESTE 4: Ferramentas de IA\n",
      "\n",
      "‚ùì Pergunta: 'OpenAI Gemini Claude LLM'\n",
      "------------------------------------------------------------\n",
      "üîç Debug - Chaves do resultado: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\n",
      "üìÑ Resultado 1 (ID: chunk_138):\n",
      "   üéØ Similaridade: -0.207 (quanto maior, mais similar)\n",
      "   üìù Texto encontrado:\n",
      "      'Full Model Name Org. Lic. Cut-off Date\n",
      "Reasoning Models\n",
      "o4-mini-high (250416) [47] OpenAI Proprietary May 31 2024\n",
      "Gemini 2.5 Pro Experimental 03-25 [22] Google Proprietary January 2025\n",
      "o3-mini (250131) [46] OpenAI Proprietary Sep 30 2023\n",
      "DeepSeek R1 [25] DeepSeek MIT Not specified\n",
      "Gemini 2.5 Flash P...'\n",
      "\n",
      "üìÑ Resultado 2 (ID: chunk_126):\n",
      "   üéØ Similaridade: -0.288 (quanto maior, mais similar)\n",
      "   üìù Texto encontrado:\n",
      "      '[34] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\n",
      "Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint\n",
      "arXiv:2412.16720, 2024.\n",
      "14...'\n",
      "\n",
      "üìÑ Resultado 3 (ID: chunk_123):\n",
      "   üéØ Similaridade: -0.295 (quanto maior, mais similar)\n",
      "   üìù Texto encontrado:\n",
      "      'LiveCodeBench Pro\n",
      "[20] Google DeepMind. Model card: Gemini 2.0 flash reasoning.https://blog.google/technology/\n",
      "google-deepmind/gemini-model-updates-february-2025/ , 2025.\n",
      "[21] Google DeepMind. Model card: Gemini 2.5 flash. https://blog.google/products/gemini/\n",
      "gemini-2-5-flash-preview/ , 2025.\n",
      "[22] G...'\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä AN√ÅLISE DOS RESULTADOS:\n",
      "   Experi√™ncia IA: -0.184\n",
      "   Kaggle: -0.103\n",
      "   C√≥digo Python: -0.143\n",
      "   Ferramentas IA: -0.207\n",
      "\n",
      "üèÜ Melhor match encontrado: -0.103\n",
      "   üîç O sistema encontrou algo, mas pode n√£o ser muito espec√≠fico\n",
      "\n",
      "‚ú® Buscas sem√¢nticas conclu√≠das!\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 10: Buscas sem√¢nticas - VERS√ÉO CORRIGIDA\n",
    "print(\"üîç Primeira Busca Sem√¢ntica Real\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Verificar se temos o banco vetorial\n",
    "try:\n",
    "    total_items = collection.count()\n",
    "    print(f\"‚úÖ Banco vetorial dispon√≠vel: {total_items} chunks\")\n",
    "except:\n",
    "    print(\"‚ùå Banco vetorial n√£o encontrado!\")\n",
    "    exit()\n",
    "\n",
    "# Fun√ß√£o corrigida para fazer buscas\n",
    "def buscar_documentos(pergunta, num_resultados=3):\n",
    "    print(f\"\\n‚ùì Pergunta: '{pergunta}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Fazer a busca\n",
    "        resultados = collection.query(\n",
    "            query_texts=[pergunta],\n",
    "            n_results=num_resultados\n",
    "        )\n",
    "        \n",
    "        # Debug: mostrar estrutura do resultado\n",
    "        print(f\"üîç Debug - Chaves do resultado: {list(resultados.keys())}\")\n",
    "        \n",
    "        # Extrair informa√ß√µes de forma segura\n",
    "        documentos_encontrados = resultados.get('documents', [[]])[0]\n",
    "        ids = resultados.get('ids', [[]])[0]\n",
    "        \n",
    "        # ChromaDB pode retornar 'distances' ou n√£o, vamos verificar\n",
    "        if 'distances' in resultados:\n",
    "            distancias = resultados['distances'][0]\n",
    "        else:\n",
    "            # Se n√£o tem distances, simular com valores neutros\n",
    "            distancias = [0.5] * len(documentos_encontrados)\n",
    "            print(\"‚ö†Ô∏è Distances n√£o dispon√≠veis, usando valores simulados\")\n",
    "        \n",
    "        # Verificar se temos resultados\n",
    "        if not documentos_encontrados:\n",
    "            print(\"‚ùå Nenhum resultado encontrado!\")\n",
    "            return [], []\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        for i, (doc, distancia, id_chunk) in enumerate(zip(documentos_encontrados, distancias, ids)):\n",
    "            similaridade = 1 - distancia  # Converter dist√¢ncia em similaridade\n",
    "            print(f\"üìÑ Resultado {i+1} (ID: {id_chunk}):\")\n",
    "            print(f\"   üéØ Similaridade: {similaridade:.3f} (quanto maior, mais similar)\")\n",
    "            print(f\"   üìù Texto encontrado:\")\n",
    "            print(f\"      '{doc[:300]}...'\")\n",
    "            print()\n",
    "        \n",
    "        return documentos_encontrados, distancias\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro na busca: {e}\")\n",
    "        return [], []\n",
    "\n",
    "# Teste 1: Buscar informa√ß√µes sobre experi√™ncia profissional\n",
    "print(\"üß™ TESTE 1: Experi√™ncia Profissional\")\n",
    "docs1, dist1 = buscar_documentos(\"experi√™ncia com intelig√™ncia artificial e machine learning\")\n",
    "\n",
    "# Teste 2: Buscar sobre Kaggle (mencionado no seu background)\n",
    "print(\"\\nüß™ TESTE 2: Competi√ß√µes e Kaggle\") \n",
    "docs2, dist2 = buscar_documentos(\"participa√ß√£o em competi√ß√£o Kaggle\")\n",
    "\n",
    "# Teste 3: Buscar sobre c√≥digo/programa√ß√£o\n",
    "print(\"\\nüß™ TESTE 3: C√≥digo e Programa√ß√£o\")\n",
    "docs3, dist3 = buscar_documentos(\"c√≥digo Python machine learning\")\n",
    "\n",
    "# Teste 4: Buscar sobre ferramentas de IA\n",
    "print(\"\\nüß™ TESTE 4: Ferramentas de IA\")\n",
    "docs4, dist4 = buscar_documentos(\"OpenAI Gemini Claude LLM\")\n",
    "\n",
    "# An√°lise de resultados (s√≥ se temos dados v√°lidos)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä AN√ÅLISE DOS RESULTADOS:\")\n",
    "\n",
    "todos_testes = [\n",
    "    (\"Experi√™ncia IA\", dist1),\n",
    "    (\"Kaggle\", dist2), \n",
    "    (\"C√≥digo Python\", dist3),\n",
    "    (\"Ferramentas IA\", dist4)\n",
    "]\n",
    "\n",
    "similaridades_validas = []\n",
    "for nome, distancias in todos_testes:\n",
    "    if distancias:  # Se tem resultados\n",
    "        melhor_similaridade = 1 - min(distancias)\n",
    "        similaridades_validas.append(melhor_similaridade)\n",
    "        print(f\"   {nome}: {melhor_similaridade:.3f}\")\n",
    "    else:\n",
    "        print(f\"   {nome}: Sem resultados\")\n",
    "\n",
    "if similaridades_validas:\n",
    "    melhor_busca = max(similaridades_validas)\n",
    "    print(f\"\\nüèÜ Melhor match encontrado: {melhor_busca:.3f}\")\n",
    "    \n",
    "    if melhor_busca > 0.8:\n",
    "        print(\"   üíö Excelente! O sistema entendeu muito bem a pergunta\")\n",
    "    elif melhor_busca > 0.6:\n",
    "        print(\"   üíõ Bom! O sistema encontrou conte√∫do relevante\")\n",
    "    else:\n",
    "        print(\"   üîç O sistema encontrou algo, mas pode n√£o ser muito espec√≠fico\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum resultado v√°lido encontrado\")\n",
    "\n",
    "print(\"\\n‚ú® Buscas sem√¢nticas conclu√≠das!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fab857-021f-460d-a493-d632a51f840d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain RAG",
   "language": "python",
   "name": "langchain-rag-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
