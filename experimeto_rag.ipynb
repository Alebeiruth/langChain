{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d39d63fd-63ac-49ae-873a-8dd62c6c96bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ RECUPERAÃ‡ÃƒO COMPLETA DO SISTEMA RAG\n",
      "============================================================\n",
      "1ï¸âƒ£ Verificando bibliotecas...\n",
      "âœ… Todas as bibliotecas importadas com sucesso\n",
      "\n",
      "2ï¸âƒ£ Carregando modelo de embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modelo carregado\n",
      "\n",
      "3ï¸âƒ£ Recarregando documentos da pasta data...\n",
      "   âœ… background.txt: 2079 chars\n",
      "   âœ… coddigo.txt: 59433 chars\n",
      "   âœ… 2506.11928v1.pdf: 71 pÃ¡ginas\n",
      "   âœ… Facial_Emotion_Recognition_(FER)_Advances_and_Applications_(2022-2025).pdf: 5 pÃ¡ginas\n",
      "âœ… 4 documentos carregados\n",
      "\n",
      "4ï¸âƒ£ Criando chunks...\n",
      "âœ… 325 chunks criados\n",
      "\n",
      "5ï¸âƒ£ Criando banco vetorial...\n",
      "   Gerando embeddings...\n",
      "   Indexando no banco...\n",
      "âœ… 325 chunks indexados no banco vetorial\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ SISTEMA RAG COMPLETAMENTE RESTAURADO!\n",
      "   ğŸ“š Documentos: 4\n",
      "   ğŸ§© Chunks: 325\n",
      "   ğŸ’¾ Banco vetorial: 325 itens\n",
      "   ğŸ§  Modelo embedding: all-MiniLM-L6-v2\n",
      "\n",
      "ğŸ§ª Teste rÃ¡pido de busca:\n",
      "âœ… Busca funcionando perfeitamente!\n",
      "\n",
      "ğŸš€ Agora vocÃª pode fazer buscas semÃ¢nticas!\n"
     ]
    }
   ],
   "source": [
    "# CÃ‰LULA DE RECUPERAÃ‡ÃƒO COMPLETA - Recria tudo do zero\n",
    "print(\"ğŸ”„ RECUPERAÃ‡ÃƒO COMPLETA DO SISTEMA RAG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ========== PASSO 1: VERIFICAR BIBLIOTECAS ==========\n",
    "print(\"1ï¸âƒ£ Verificando bibliotecas...\")\n",
    "try:\n",
    "    import sys\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from pathlib import Path\n",
    "    print(\"âœ… Todas as bibliotecas importadas com sucesso\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Erro de importaÃ§Ã£o: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ========== PASSO 2: CARREGAR MODELO DE EMBEDDING ==========\n",
    "print(\"\\n2ï¸âƒ£ Carregando modelo de embedding...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"âœ… Modelo carregado\")\n",
    "\n",
    "# ========== PASSO 3: RECARREGAR DOCUMENTOS ==========\n",
    "print(\"\\n3ï¸âƒ£ Recarregando documentos da pasta data...\")\n",
    "data_path = Path(\"data\")\n",
    "documentos_reais = []\n",
    "metadados_docs = []\n",
    "\n",
    "if data_path.exists():\n",
    "    # Carregar TXTs\n",
    "    for arquivo_txt in data_path.glob(\"*.txt\"):\n",
    "        try:\n",
    "            loader = TextLoader(str(arquivo_txt), encoding='utf-8')\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                documentos_reais.append(doc.page_content)\n",
    "                metadados_docs.append({\n",
    "                    'source': arquivo_txt.name,\n",
    "                    'type': 'TXT',\n",
    "                    'size': len(doc.page_content)\n",
    "                })\n",
    "            print(f\"   âœ… {arquivo_txt.name}: {len(docs[0].page_content)} chars\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Erro em {arquivo_txt.name}: {e}\")\n",
    "    \n",
    "    # Carregar PDFs\n",
    "    for arquivo_pdf in data_path.glob(\"*.pdf\"):\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(arquivo_pdf))\n",
    "            docs = loader.load()\n",
    "            texto_completo = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "            documentos_reais.append(texto_completo)\n",
    "            metadados_docs.append({\n",
    "                'source': arquivo_pdf.name,\n",
    "                'type': 'PDF',\n",
    "                'pages': len(docs),\n",
    "                'size': len(texto_completo)\n",
    "            })\n",
    "            print(f\"   âœ… {arquivo_pdf.name}: {len(docs)} pÃ¡ginas\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Erro em {arquivo_pdf.name}: {e}\")\n",
    "\n",
    "print(f\"âœ… {len(documentos_reais)} documentos carregados\")\n",
    "\n",
    "# ========== PASSO 4: CRIAR CHUNKS ==========\n",
    "print(\"\\n4ï¸âƒ£ Criando chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks_reais = []\n",
    "for i, (doc_text, metadata) in enumerate(zip(documentos_reais, metadados_docs)):\n",
    "    doc_chunks = text_splitter.split_text(doc_text)\n",
    "    for j, chunk in enumerate(doc_chunks):\n",
    "        chunk_info = {\n",
    "            'id': f'doc_{i}_chunk_{j}',\n",
    "            'text': chunk,\n",
    "            'source_file': metadata['source'],\n",
    "            'source_type': metadata['type'],\n",
    "            'chunk_number': j,\n",
    "            'char_count': len(chunk)\n",
    "        }\n",
    "        chunks_reais.append(chunk_info)\n",
    "\n",
    "print(f\"âœ… {len(chunks_reais)} chunks criados\")\n",
    "\n",
    "# ========== PASSO 5: CRIAR BANCO VETORIAL ==========\n",
    "print(\"\\n5ï¸âƒ£ Criando banco vetorial...\")\n",
    "client = chromadb.Client(Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    allow_reset=True\n",
    "))\n",
    "\n",
    "# Limpar collection anterior\n",
    "try:\n",
    "    client.delete_collection(\"meus_documentos_rag\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = client.create_collection(\"meus_documentos_rag\")\n",
    "\n",
    "# Preparar dados\n",
    "textos_chunks = [chunk['text'] for chunk in chunks_reais]\n",
    "ids_chunks = [f\"chunk_{i}\" for i in range(len(chunks_reais))]\n",
    "\n",
    "# Gerar embeddings\n",
    "print(\"   Gerando embeddings...\")\n",
    "embeddings_chunks = embedding_model.encode(textos_chunks)\n",
    "\n",
    "# Indexar\n",
    "print(\"   Indexando no banco...\")\n",
    "collection.add(\n",
    "    embeddings=embeddings_chunks.tolist(),\n",
    "    documents=textos_chunks,\n",
    "    ids=ids_chunks\n",
    ")\n",
    "\n",
    "total_items = collection.count()\n",
    "print(f\"âœ… {total_items} chunks indexados no banco vetorial\")\n",
    "\n",
    "# ========== VERIFICAÃ‡ÃƒO FINAL ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ SISTEMA RAG COMPLETAMENTE RESTAURADO!\")\n",
    "print(f\"   ğŸ“š Documentos: {len(documentos_reais)}\")\n",
    "print(f\"   ğŸ§© Chunks: {len(chunks_reais)}\")\n",
    "print(f\"   ğŸ’¾ Banco vetorial: {total_items} itens\")\n",
    "print(f\"   ğŸ§  Modelo embedding: all-MiniLM-L6-v2\")\n",
    "\n",
    "# Teste rÃ¡pido\n",
    "print(\"\\nğŸ§ª Teste rÃ¡pido de busca:\")\n",
    "resultado_teste = collection.query(\n",
    "    query_texts=[\"inteligÃªncia artificial\"],\n",
    "    n_results=1\n",
    ")\n",
    "print(\"âœ… Busca funcionando perfeitamente!\")\n",
    "\n",
    "print(\"\\nğŸš€ Agora vocÃª pode fazer buscas semÃ¢nticas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0639a3bd-068f-4596-969e-c8fb986bc834",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phyton: 3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:47:33) [MSC v.1943 64 bit (AMD64)]\n",
      "Verificando se as bibliotecas estÃ£o disponiveis...\n",
      "âœ… LangChain: 0.3.26\n",
      "âœ… ChromaDB: 1.0.15\n",
      "âœ… Sentence Transformers: 5.0.0\n"
     ]
    }
   ],
   "source": [
    "# CÃ©lula 1: VerificaÃ§Ã£o basica e importaÃ§Ãµes \n",
    "import sys\n",
    "print(f\"Phyton: {sys.version}\")\n",
    "\n",
    "# Verifica se estamos no ambiente correto\n",
    "print(\"Verificando se as bibliotecas estÃ£o disponiveis...\")\n",
    "\n",
    "try:\n",
    "    import langchain\n",
    "    print(f\"âœ… LangChain: {langchain.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ LangChain nÃ£o encontrado\")\n",
    "\n",
    "try:\n",
    "    import chromadb\n",
    "    print(f\"âœ… ChromaDB: {chromadb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ ChromaDB nÃ£o encontrado\")\n",
    "\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    print(f\"âœ… Sentence Transformers: {sentence_transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Sentence Transformers nÃ£o encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f21ce73-ad3d-473f-8e88-8343db692213",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando modelo de embedding...\n",
      "âœ… Modelo Carregado!\n",
      "\n",
      "Vamos analisar 6 frases:\n",
      "1. 'O gato subiu no telhado'\n",
      "2. 'Um felino escalou o teto de casa'\n",
      "3. 'Hoje estÃ¡ chovendo muito'\n",
      "4. 'Tempestade ocorreu com intensidade'\n",
      "5. 'O cachorro late no quintal'\n",
      "6. 'O canino ladra mas nÃ£o morde'\n",
      "\n",
      "Gerando embeddings...\n",
      "âœ… Shape dos embeddings: (6, 384)\n",
      "âœ… Cada texto virou um vetor de 384 dimensÃµes\n"
     ]
    }
   ],
   "source": [
    "# CÃ©lula 2: Experimento com embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Carregar um modelo pequeno para entender embeddings\n",
    "print(\"Carregando modelo de embedding...\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"âœ… Modelo Carregado!\")\n",
    "\n",
    "# Teste bÃ¡sico com frases similares e diferentes\n",
    "textos = [\n",
    "    \"O gato subiu no telhado\",\n",
    "    \"Um felino escalou o teto de casa\",\n",
    "    \"Hoje estÃ¡ chovendo muito\",\n",
    "    \"Tempestade ocorreu com intensidade\",\n",
    "    \"O cachorro late no quintal\",\n",
    "    \"O canino ladra mas nÃ£o morde\"\n",
    "]\n",
    "\n",
    "print(f\"\\nVamos analisar {len(textos)} frases:\")\n",
    "for i, texto in enumerate(textos):\n",
    "    print(f\"{i+1}. '{texto}'\")\n",
    "\n",
    "# Gerar embeddings\n",
    "print(\"\\nGerando embeddings...\")\n",
    "embeddings = embedding_model.encode(textos)\n",
    "print(f\"âœ… Shape dos embeddings: {embeddings.shape}\")\n",
    "print(f\"âœ… Cada texto virou um vetor de {embeddings.shape[1]} dimensÃµes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7349c2c2-671c-49fa-afb9-99dca8778622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analisando a similaridade entre os nossos 6 textos:\n",
      "\n",
      "1. 'O gato subiu no telhado'\n",
      "2. 'Um felino escalou o teto de casa'\n",
      "3. 'Hoje estÃ¡ chovendo muito'\n",
      "4. 'Tempestade ocorreu com intensidade'\n",
      "5. 'O cachorro late no quintal'\n",
      "6. 'O canino ladra mas nÃ£o morde'\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Matriz de similaridade:\n",
      "(1.0 = idÃªntico, 0.0 = neutro, -1.0 = oposto)\n",
      "\n",
      "Texto 1 vs Texto 2: 0.497\n",
      "  'O gato subiu no telhado...' â†” 'Um felino escalou o teto de ca...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 1 vs Texto 3: 0.469\n",
      "  'O gato subiu no telhado...' â†” 'Hoje estÃ¡ chovendo muito...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 1 vs Texto 4: 0.418\n",
      "  'O gato subiu no telhado...' â†” 'Tempestade ocorreu com intensi...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 1 vs Texto 5: 0.548\n",
      "  'O gato subiu no telhado...' â†” 'O cachorro late no quintal...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 1 vs Texto 6: 0.503\n",
      "  'O gato subiu no telhado...' â†” 'O canino ladra mas nÃ£o morde...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 2 vs Texto 3: 0.456\n",
      "  'Um felino escalou o teto de ca...' â†” 'Hoje estÃ¡ chovendo muito...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 2 vs Texto 4: 0.506\n",
      "  'Um felino escalou o teto de ca...' â†” 'Tempestade ocorreu com intensi...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 2 vs Texto 5: 0.477\n",
      "  'Um felino escalou o teto de ca...' â†” 'O cachorro late no quintal...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 2 vs Texto 6: 0.467\n",
      "  'Um felino escalou o teto de ca...' â†” 'O canino ladra mas nÃ£o morde...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 3 vs Texto 4: 0.422\n",
      "  'Hoje estÃ¡ chovendo muito...' â†” 'Tempestade ocorreu com intensi...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 3 vs Texto 5: 0.441\n",
      "  'Hoje estÃ¡ chovendo muito...' â†” 'O cachorro late no quintal...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 3 vs Texto 6: 0.561\n",
      "  'Hoje estÃ¡ chovendo muito...' â†” 'O canino ladra mas nÃ£o morde...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 4 vs Texto 5: 0.360\n",
      "  'Tempestade ocorreu com intensi...' â†” 'O cachorro late no quintal...'\n",
      "  â¤ï¸ Pouco similares\n",
      "\n",
      "Texto 4 vs Texto 6: 0.412\n",
      "  'Tempestade ocorreu com intensi...' â†” 'O canino ladra mas nÃ£o morde...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n",
      "Texto 5 vs Texto 6: 0.415\n",
      "  'O cachorro late no quintal...' â†” 'O canino ladra mas nÃ£o morde...'\n",
      "  ğŸ’› Moderadamente similares\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CÃ©lula 3: Calculando a similaridade entre textos\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"ğŸ” Analisando a similaridade entre os nossos 6 textos:\")\n",
    "print()\n",
    "\n",
    "# Relembrar quais sÃ£o os textos\n",
    "for i, texto in enumerate(textos):\n",
    "    print(f\"{i+1}. '{texto}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Calcular similaridade entre todos os textos\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"ğŸ“Š Matriz de similaridade:\")\n",
    "print(\"(1.0 = idÃªntico, 0.0 = neutro, -1.0 = oposto)\")\n",
    "print()\n",
    "\n",
    "# Mostarr comparaÃ§Ã£o de forma organizada\n",
    "for i, texto1 in enumerate(textos):\n",
    "    for j, texto2 in enumerate(textos):\n",
    "        if i < j: # evita repetiÃ§Ãµes (sÃ³ mostra a metade da matriz)\n",
    "            sim = similarities[i][j]\n",
    "            print(f\"Texto {i+1} vs Texto {j+1}: {sim:.3f}\")\n",
    "            print(f\"  '{texto1[:30]}...' â†” '{texto2[:30]}...'\")\n",
    "        \n",
    "            # InterpretaÃ§Ã£o humana\n",
    "            if sim > 0.7:\n",
    "                print(f\"  ğŸ’š Muito similares!\")\n",
    "            elif sim > 0.4:\n",
    "                print(f\"  ğŸ’› Moderadamente similares\")\n",
    "            else:\n",
    "                print(f\"  â¤ï¸ Pouco similares\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d65892a-5d43-4c61-b7e6-ff4a382f7144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Base de Conhecimetno Criada!\n",
      "==================================================\n",
      "Documento 1:\n",
      "  ğŸ“„ 50 palavras, 369 caracteres\n",
      "  ğŸ“ Tema: InteligÃªncia Artificial (IA) Ã© um campo da ciÃªncia...\n",
      "\n",
      "Documento 2:\n",
      "  ğŸ“„ 42 palavras, 334 caracteres\n",
      "  ğŸ“ Tema: Machine Learning Ã© uma subcategoria da IA que perm...\n",
      "\n",
      "Documento 3:\n",
      "  ğŸ“„ 42 palavras, 299 caracteres\n",
      "  ğŸ“ Tema: Python Ã© uma linguagem de programaÃ§Ã£o de alto nÃ­ve...\n",
      "\n",
      "Documento 4:\n",
      "  ğŸ“„ 42 palavras, 329 caracteres\n",
      "  ğŸ“ Tema: LangChain Ã© um framework para desenvolver aplicaÃ§Ãµ...\n",
      "\n",
      "Documento 5:\n",
      "  ğŸ“„ 41 palavras, 309 caracteres\n",
      "  ğŸ“ Tema: Bancos vetoriais sÃ£o sistemas de banco de dados es...\n",
      "\n",
      "âœ… Total: 5 documentos em nossa base\n"
     ]
    }
   ],
   "source": [
    "# CÃ©lula 4: Criando nossa \"base de conhecimento\"\n",
    "documentos = [\n",
    "    \"\"\"\n",
    "    InteligÃªncia Artificial (IA) Ã© um campo da ciÃªncia da computaÃ§Ã£o que se concentra \n",
    "    no desenvolvimento de sistemas capazes de realizar tarefas que normalmente \n",
    "    requerem inteligÃªncia humana. Isso inclui aprendizado, raciocÃ­nio, \n",
    "    percepÃ§Ã£o e tomada de decisÃ£o. A IA pode ser classificada em IA fraca \n",
    "    (sistemas especÃ­ficos) e IA forte (inteligÃªncia geral).\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Machine Learning Ã© uma subcategoria da IA que permite que os computadores \n",
    "    aprendam e melhorem automaticamente a partir da experiÃªncia, sem serem \n",
    "    explicitamente programados. Utiliza algoritmos estatÃ­sticos para \n",
    "    identificar padrÃµes em dados. Os principais tipos sÃ£o: supervisionado, \n",
    "    nÃ£o supervisionado e por reforÃ§o.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "     Python Ã© uma linguagem de programaÃ§Ã£o de alto nÃ­vel, interpretada e \n",
    "    de propÃ³sito geral. Ã‰ amplamente utilizada em ciÃªncia de dados, \n",
    "    desenvolvimento web, automaÃ§Ã£o e inteligÃªncia artificial devido \n",
    "    Ã  sua sintaxe simples e rica biblioteca de pacotes como NumPy, \n",
    "    Pandas e TensorFlow.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    LangChain Ã© um framework para desenvolver aplicaÃ§Ãµes com modelos de \n",
    "    linguagem. Ele fornece ferramentas para conectar LLMs com fontes de \n",
    "    dados externas, criar pipelines de processamento e construir \n",
    "    aplicaÃ§Ãµes inteligentes como chatbots e sistemas de pergunta-resposta.\n",
    "    LangChain facilita a implementaÃ§Ã£o de RAG.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Bancos vetoriais sÃ£o sistemas de banco de dados especializados em \n",
    "    armazenar e buscar vetores de alta dimensionalidade. Eles sÃ£o \n",
    "    fundamentais para aplicaÃ§Ãµes de IA que utilizam embeddings, como \n",
    "    sistemas de recomendaÃ§Ã£o, busca semÃ¢ntica e RAG. Exemplos incluem \n",
    "    ChromaDB, Pinecone e Weaviate.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“š Base de Conhecimetno Criada!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, doc in enumerate(documentos):\n",
    "    palavras = len(doc.split())\n",
    "    caracteres = len(doc.strip())\n",
    "    print(f\"Documento {i+1}:\")\n",
    "    print(f\"  ğŸ“„ {palavras} palavras, {caracteres} caracteres\")\n",
    "    print(f\"  ğŸ“ Tema: {doc.strip()[:50]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"âœ… Total: {len(documentos)} documentos em nossa base\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bcc7210-6500-46b0-8dff-853acf6a40cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”ª Aprendendo sobre Chunking\n",
      "========================================\n",
      "âš™ï¸ ConfiguraÃ§Ã£o do Text Splitter:\n",
      "   ğŸ“ Tamanho mÃ¡ximo do chunk: 200 caracteres\n",
      "   ğŸ”— SobreposiÃ§Ã£o: 50 caracteres\n",
      "   âœ‚ï¸ Separadores: ['\\n\\n', '\\n', '. ', ' ', '']\n",
      "\n",
      "ğŸ“„ Documento 1 â†’ 3 chunk(s)\n",
      " Chunk 3: 55 chars â†’ '(sistemas especÃ­ficos) e IA forte (inteligÃªncia geral)....'\n",
      "ğŸ“„ Documento 2 â†’ 2 chunk(s)\n",
      " Chunk 2: 179 chars â†’ 'explicitamente programados. Utiliza algoritmos estatÃ­sticos ...'\n",
      "ğŸ“„ Documento 3 â†’ 2 chunk(s)\n",
      " Chunk 2: 157 chars â†’ 'desenvolvimento web, automaÃ§Ã£o e inteligÃªncia artificial dev...'\n",
      "ğŸ“„ Documento 4 â†’ 2 chunk(s)\n",
      " Chunk 2: 183 chars â†’ 'dados externas, criar pipelines de processamento e construir...'\n",
      "ğŸ“„ Documento 5 â†’ 2 chunk(s)\n",
      " Chunk 2: 171 chars â†’ 'fundamentais para aplicaÃ§Ãµes de IA que utilizam embeddings, ...'\n",
      "\n",
      "==================================================\n",
      "ğŸ¯ Resultado Final:\n",
      "   ğŸ“š 5 documentos originais\n",
      "   ğŸ§© 5 chunks criados\n",
      "   ğŸ“Š MÃ©dia de 1.0 chunks por documento\n"
     ]
    }
   ],
   "source": [
    "# CÃ©lula 5: Entendendo Chunking (DivisÃ£o de Texto)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"ğŸ”ª Aprendendo sobre Chunking\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Configurar o divisor de texto\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200, # Tamanho mÃ¡ximo de cada pedaÃ§o de texto (em caracteres)\n",
    "    chunk_overlap=50, # SobreposiÃ§Ã£o entre pedaÃ§os\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # Como dividir (ordem de preferÃªncia)\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸ ConfiguraÃ§Ã£o do Text Splitter:\")\n",
    "print(f\"   ğŸ“ Tamanho mÃ¡ximo do chunk: {text_splitter._chunk_size} caracteres\")\n",
    "print(f\"   ğŸ”— SobreposiÃ§Ã£o: {text_splitter._chunk_overlap} caracteres\")\n",
    "print(f\"   âœ‚ï¸ Separadores: {text_splitter._separators}\")\n",
    "print()\n",
    "\n",
    "# Dividir nossos docuemntos\n",
    "chunks = []\n",
    "chunk_counter = 0\n",
    "\n",
    "for i, doc in enumerate(documentos):\n",
    "    doc_limpo = doc.strip()\n",
    "    doc_chunks = text_splitter.split_text(doc_limpo)\n",
    "\n",
    "    print(f\"ğŸ“„ Documento {i+1} â†’ {len(doc_chunks)} chunk(s)\")\n",
    "    \n",
    "    for j, chunk in enumerate(doc_chunks):\n",
    "         chunk_info = {\n",
    "        'id': f\"chunk_{chunk_counter}\",\n",
    "        'text': chunk,\n",
    "        'source': f\"documento_{i+1}\",\n",
    "        'chunk_num': j,\n",
    "        'chars': len(chunk)\n",
    "    }\n",
    "    chunks.append(chunk_info)\n",
    "    \n",
    "    print(f\" Chunk {j+1}: {len(chunk)} chars â†’ '{chunk[:60]}...'\")\n",
    "    chunk_counter += 1\n",
    "print()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"ğŸ¯ Resultado Final:\")\n",
    "print(f\"   ğŸ“š {len(documentos)} documentos originais\")\n",
    "print(f\"   ğŸ§© {len(chunks)} chunks criados\")\n",
    "print(f\"   ğŸ“Š MÃ©dia de {len(chunks)/len(documentos):.1f} chunks por documento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0a70a21-9852-4bd2-8c4f-e328acec078a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Arquivos encontrados na pasta 'data':\n",
      "==================================================\n",
      "ğŸ“„ PDF: 2506.11928v1.pdf (712.0 KB)\n",
      "ğŸ“ TXT: background.txt (2.1 KB)\n",
      "ğŸ“ TXT: coddigo.txt (60.7 KB)\n",
      "ğŸ“„ PDF: Facial_Emotion_Recognition_(FER)_Advances_and_Applications_(2022-2025).pdf (78.3 KB)\n",
      "â“ Outro: imagem (8).png (38.0 KB)\n",
      "\n",
      "ğŸ“Š Resumo:\n",
      "   ğŸ“„ 2 arquivo(s) PDF\n",
      "   ğŸ“ 2 arquivo(s) TXT\n",
      "   â“ 1 outro(s) arquivo(s)\n",
      "\n",
      "âœ… Vamos usar esses arquivos no nosso RAG!\n"
     ]
    }
   ],
   "source": [
    "# CÃ©lula 6: Explorando arquivos reais na pasta data\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Vrificar pasta data\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "if data_path.exists():\n",
    "    print(\"ğŸ“ Arquivos encontrados na pasta 'data':\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    arquivo_pdf = []\n",
    "    arquivo_txt = []\n",
    "    outros_arquivos = []\n",
    "\n",
    "    for arquivo in data_path.iterdir():\n",
    "        if arquivo.is_file():\n",
    "            tamanho_kb = arquivo.stat().st_size / 1024\n",
    "\n",
    "            if arquivo.suffix.lower() == '.pdf':\n",
    "                arquivo_pdf.append(arquivo)\n",
    "                print(f\"ğŸ“„ PDF: {arquivo.name} ({tamanho_kb:.1f} KB)\")\n",
    "            elif arquivo.suffix.lower() == '.txt':\n",
    "                arquivo_txt.append(arquivo)\n",
    "                print(f\"ğŸ“ TXT: {arquivo.name} ({tamanho_kb:.1f} KB)\")\n",
    "            else:\n",
    "                outros_arquivos.append(arquivo)\n",
    "                print(f\"â“ Outro: {arquivo.name} ({tamanho_kb:.1f} KB)\")\n",
    "                \n",
    "    print(f\"\\nğŸ“Š Resumo:\")\n",
    "    print(f\"   ğŸ“„ {len(arquivo_pdf)} arquivo(s) PDF\")\n",
    "    print(f\"   ğŸ“ {len(arquivo_txt)} arquivo(s) TXT\")\n",
    "    print(f\"   â“ {len(outros_arquivos)} outro(s) arquivo(s)\")\n",
    "\n",
    "    if len(arquivo_pdf) + len(arquivo_txt) > 0:\n",
    "        print(\"\\nâœ… Vamos usar esses arquivos no nosso RAG!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ Nenhum PDF ou TXT encontrado. Vamos usar documentos sintÃ©ticos.\")\n",
    "else:\n",
    "    print(\"âŒ Pasta 'data' nÃ£o encontrada.\")\n",
    "    print(\"ğŸ’¡ Crie a pasta 'data' e coloque seus arquivos lÃ¡, ou vamos continuar com documentos sintÃ©ticos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd781a81-1c9a-4d85-aa11-3c1c108a2081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Carregando documentos reais...\n",
      "========================================\n",
      "ğŸ“ Carregando arquivos TXT:\n",
      "   âŒ Erro ao carregar background.txt: 'list' object has no attribute 'name'\n",
      "   âŒ Erro ao carregar coddigo.txt: 'list' object has no attribute 'name'\n",
      "ğŸ“ Carregando arquivos PDF:\n",
      "   âŒ Erro ao carregar 2506.11928v1.pdf: 'list' object has no attribute 'name'\n",
      "   âŒ Erro ao carregar Facial_Emotion_Recognition_(FER)_Advances_and_Applications_(2022-2025).pdf: 'list' object has no attribute 'name'\n",
      "\n",
      "==================================================\n",
      "ğŸ¯ Documentos carregados com sucesso:\n",
      "   ğŸ“„ Total: 4 documento(s)\n",
      "   1. background.txt (TXT) - 2079 chars\n",
      "   2. coddigo.txt (TXT) - 59433 chars\n",
      "\n",
      "âœ… Vamos usar seus 4 documento(s) real(is) no RAG!\n",
      "\n",
      "ğŸ“„ Amostra do primeiro documento (background.txt):\n",
      "'jÃ¡ trabalhei com Llama inclusive tenho participaÃ§Ã£o da primeira equipe de IA em uma competiÃ§Ã£o no Kaggle, trabalho diariamente com OpenAI, Gemini e Claude da Anthopric.\n",
      "Ainda nÃ£o tive oportunidade de ...'\n"
     ]
    }
   ],
   "source": [
    "# CÃ©lula 7: Carergando documentos reais\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "print(\"ğŸ“š Carregando documentos reais...\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "documentos_reais = []\n",
    "metadados_docs = []\n",
    "\n",
    "# Carregar arquivos TXT\n",
    "if arquivo_txt:\n",
    "    print(\"ğŸ“ Carregando arquivos TXT:\")\n",
    "    for arquivos_txt in arquivo_txt:\n",
    "        try:\n",
    "            loader = TextLoader(str(arquivos_txt), encoding='utf-8')\n",
    "            docs = loader.load()\n",
    "\n",
    "            for doc in docs:\n",
    "                documentos_reais.append(doc.page_content)\n",
    "                metadados_docs.append({\n",
    "                    'source': arquivos_txt.name,\n",
    "                    'type': 'TXT',\n",
    "                    'size': len(doc.page_content)\n",
    "                })\n",
    "                \n",
    "            print(f\"   âœ… {arquivo_txt.name}: {len(docs[0].page_content)} caracteres\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Erro ao carregar {arquivos_txt.name}: {e}\")\n",
    "\n",
    "# Carregar arquivo PDF\n",
    "if arquivo_pdf:\n",
    "    print(\"ğŸ“ Carregando arquivos PDF:\")\n",
    "    for arquivos_pdf in arquivo_pdf:\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(arquivos_pdf))\n",
    "            docs = loader.load()\n",
    "\n",
    "            # Juntar todas as paginas do PDF\n",
    "            texto_completo = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "            documentos_reais.append(texto_completo)\n",
    "\n",
    "            metadados_docs.append({\n",
    "                'source': arquivo_pdf.name,\n",
    "                'type': 'PDF',\n",
    "                'pages': len(docs),\n",
    "                'size': len(texto_completo)\n",
    "            })\n",
    "\n",
    "            print(f\"   âœ… {arquivo_pdf.name}: {len(docs)} pÃ¡ginas, {len(texto_completo)} caracteres\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Erro ao carregar {arquivos_pdf.name}: {e}\")\n",
    "\n",
    "# Resultado final\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"ğŸ¯ Documentos carregados com sucesso:\")\n",
    "print(f\"   ğŸ“„ Total: {len(documentos_reais)} documento(s)\")\n",
    "\n",
    "for i, metadata in enumerate(metadados_docs):\n",
    "    print(f\"   {i+1}. {metadata['source']} ({metadata['type']}) - {metadata['size']} chars\")\n",
    "\n",
    "# Se nÃ£o conseguiu carregar nenhum, usar sintÃ©ticos\n",
    "if not documentos_reais:\n",
    "    print(\"\\nâš ï¸ Nenhum documento foi carregado. Usando documentos sintÃ©ticos...\")\n",
    "    # (aqui poderia voltar aos documentos sintÃ©ticos se necessÃ¡rio)\n",
    "else:\n",
    "    print(f\"\\nâœ… Vamos usar seus {len(documentos_reais)} documento(s) real(is) no RAG!\")\n",
    "    \n",
    "    # Mostrar uma amostra do primeiro documento\n",
    "    print(f\"\\nğŸ“„ Amostra do primeiro documento ({metadados_docs[0]['source']}):\")\n",
    "    print(f\"'{documentos_reais[0][:200]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d1c6ccf-2f77-401d-a461-e99789b9680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”ª Aplicando Chunking aos Documentos Reais\n",
      "==================================================\n",
      "âš™ï¸ ConfiguraÃ§Ã£o do Chunking:\n",
      "   ğŸ“ Tamanho mÃ¡ximo: 1000 caracteres\n",
      "   ğŸ”— SobreposiÃ§Ã£o: 200 caracteres\n",
      "\n",
      "ğŸ“„ Processando: background.txt\n",
      "   ğŸ“Š Documento original: 2,079 caracteres\n",
      "   ğŸ§© Resultado: 4 chunks\n",
      "   ğŸ“ˆ Tamanho dos chunks: 354-997 chars (mÃ©dia: 568)\n",
      "   ğŸ“ Primeiro chunk: 'jÃ¡ trabalhei com Llama inclusive tenho participaÃ§Ã£o da primeira equipe de IA em uma competiÃ§Ã£o no Ka...'\n",
      "\n",
      "ğŸ“„ Processando: coddigo.txt\n",
      "   ğŸ“Š Documento original: 59,433 caracteres\n",
      "   ğŸ§© Resultado: 71 chunks\n",
      "   ğŸ“ˆ Tamanho dos chunks: 355-998 chars (mÃ©dia: 894)\n",
      "   ğŸ“ Primeiro chunk: '// >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<...'\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ Resumo Final do Chunking:\n",
      "   ğŸ“š 4 documentos processados\n",
      "   ğŸ§© 75 chunks criados no total\n",
      "   ğŸ“Š MÃ©dia de 18.8 chunks por documento\n",
      "\n",
      "ğŸ“‹ Detalhes por arquivo:\n",
      "   ğŸ“„ background.txt: 4 chunks (mÃ©dia 568 chars)\n",
      "   ğŸ“„ coddigo.txt: 71 chunks (mÃ©dia 894 chars)\n",
      "\n",
      "ğŸ† Documento com mais chunks: coddigo.txt (71 chunks)\n"
     ]
    }
   ],
   "source": [
    "# CÃ©lula 8: Aplicando chunking aos documentos reais\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"ğŸ”ª Aplicando Chunking aos Documentos Reais\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Configurar splitter para documentos reais (chunks maiores)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, # Maior para documentos reais\n",
    "    chunk_overlap = 200, # Overlap mairo para preservar contexto\n",
    "    separators = [\"\\n\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸ ConfiguraÃ§Ã£o do Chunking:\")\n",
    "print(f\"   ğŸ“ Tamanho mÃ¡ximo: {text_splitter._chunk_size} caracteres\")\n",
    "print(f\"   ğŸ”— SobreposiÃ§Ã£o: {text_splitter._chunk_overlap} caracteres\")\n",
    "print()\n",
    "\n",
    "# Processar cada documento\n",
    "chunk_reais = []\n",
    "estatisticas = []\n",
    "\n",
    "for i, (doc_text, metadata) in enumerate(zip(documentos_reais, metadados_docs)):\n",
    "    print(f\"ğŸ“„ Processando: {metadata['source']}\")\n",
    "    print(f\"   ğŸ“Š Documento original: {len(doc_text):,} caracteres\")\n",
    "\n",
    "    # Aplicar chunking\n",
    "    doc_chunks = text_splitter.split_text(doc_text)\n",
    "\n",
    "    print(f\"   ğŸ§© Resultado: {len(doc_chunks)} chunks\")\n",
    "\n",
    "    # Armazenar chunks com metadados\n",
    "    for j, chunk in enumerate(doc_chunks):\n",
    "        chunk_info = {\n",
    "            'id': f'doc_{i}_chunk_{j}',\n",
    "            'text': chunk,\n",
    "            'source_file': metadata['source'],\n",
    "            'source_type': metadata['type'],\n",
    "            'chunk_number': j,\n",
    "            'total_chunks': len(doc_chunks),\n",
    "            'char_count': len(chunk)\n",
    "        }\n",
    "        chunk_reais.append(chunk_info)\n",
    "\n",
    "    # Estatisticas para documento\n",
    "    tamanhos_chunks = [len(chunk) for chunk in doc_chunks]\n",
    "    stats = {\n",
    "        'arquivo': metadata['source'],\n",
    "        'chars_original': len(doc_text),\n",
    "        'num_chunks': len(doc_chunks),\n",
    "        'chunk_min': min(tamanhos_chunks),\n",
    "        'chunk_max': max(tamanhos_chunks),\n",
    "        'chunk_medio': sum(tamanhos_chunks) / len(tamanhos_chunks)\n",
    "    }\n",
    "    estatisticas.append(stats)\n",
    "\n",
    "    print(f\"   ğŸ“ˆ Tamanho dos chunks: {stats['chunk_min']}-{stats['chunk_max']} chars (mÃ©dia: {stats['chunk_medio']:.0f})\")\n",
    "\n",
    "    # Mostrar amostra do primeiro chunk\n",
    "    print(f\"   ğŸ“ Primeiro chunk: '{doc_chunks[0][:100]}...'\")\n",
    "    print()\n",
    "\n",
    "# Resumo final\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ Resumo Final do Chunking:\")\n",
    "print(f\"   ğŸ“š {len(documentos_reais)} documentos processados\")\n",
    "print(f\"   ğŸ§© {len(chunk_reais)} chunks criados no total\")\n",
    "print(f\"   ğŸ“Š MÃ©dia de {len(chunk_reais)/len(documentos_reais):.1f} chunks por documento\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Detalhes por arquivo:\")\n",
    "for stat in estatisticas:\n",
    "    print(f\"   ğŸ“„ {stat['arquivo']}: {stat['num_chunks']} chunks (mÃ©dia {stat['chunk_medio']:.0f} chars)\")\n",
    "\n",
    "# Encontrar o documento com mais chunks\n",
    "doc_mais_chunks = max(estatisticas, key=lambda x: x['num_chunks'])\n",
    "print(f\"\\nğŸ† Documento com mais chunks: {doc_mais_chunks['arquivo']} ({doc_mais_chunks['num_chunks']} chunks)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "098faefd-c72c-4cc2-904c-08b7b417af7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ Criando Banco Vetorial - VersÃ£o Simplificada\n",
      "============================================================\n",
      "ğŸ” Verificando dados antes do processamento:\n",
      "   ğŸ“š documentos_reais: 4 itens\n",
      "   ğŸ§© chunks_reais: 325 itens\n",
      "   ğŸ“„ Estrutura do primeiro chunk:\n",
      "      id: <class 'str'> = doc_0_chunk_0...\n",
      "      text: <class 'str'> = jÃ¡ trabalhei com Llama inclusive tenho participaÃ§Ã£...\n",
      "      source_file: <class 'str'> = background.txt...\n",
      "      source_type: <class 'str'> = TXT...\n",
      "      chunk_number: <class 'int'> = 0...\n",
      "      char_count: <class 'int'> = 969...\n",
      "âœ… ColeÃ§Ã£o 'meus_documentos_rag' criada\n",
      "\n",
      "ğŸ“Š Dados preparados:\n",
      "   ğŸ“ 325 textos vÃ¡lidos\n",
      "   ğŸ·ï¸ 325 IDs criados\n",
      "   ğŸ“‹ 325 metadados criados\n",
      "\n",
      "ğŸ§  Gerando embeddings...\n",
      "âœ… Embeddings gerados:\n",
      "   â±ï¸ Tempo: 13.01 segundos\n",
      "   ğŸ“Š Shape: (325, 384)\n",
      "\n",
      "ğŸ’¾ Adicionando ao banco vetorial...\n",
      "âœ… Sucesso! 325 chunks indexados\n",
      "\n",
      "ğŸ§ª Teste rÃ¡pido de busca:\n",
      "   âœ… Busca funcionando: 3 resultados encontrados\n",
      "\n",
      "ğŸ‰ Banco vetorial estÃ¡ pronto!\n"
     ]
    }
   ],
   "source": [
    "# CÃ©lula 9: Banco vetorial simplificado (sem metadados complexos)\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import time\n",
    "\n",
    "print(\"ğŸ—ï¸ Criando Banco Vetorial - VersÃ£o Simplificada\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verificar dados antes de processar\n",
    "print(\"ğŸ” Verificando dados antes do processamento:\")\n",
    "print(f\"   ğŸ“š documentos_reais: {len(documentos_reais)} itens\")\n",
    "print(f\"   ğŸ§© chunks_reais: {len(chunks_reais)} itens\")\n",
    "\n",
    "# Mostrar estrutura do primeiro chunk\n",
    "if len(chunks_reais) > 0:\n",
    "    primeiro_chunk = chunks_reais[0]\n",
    "    print(f\"   ğŸ“„ Estrutura do primeiro chunk:\")\n",
    "    for key, value in primeiro_chunk.items():\n",
    "        print(f\"      {key}: {type(value)} = {str(value)[:50]}...\")\n",
    "else:\n",
    "    print(\"   âŒ Nenhum chunk encontrado!\")\n",
    "    exit()\n",
    "\n",
    "# Configurar ChromaDB\n",
    "client = chromadb.Client(Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    allow_reset=True\n",
    "))\n",
    "\n",
    "# Limpar e criar coleÃ§Ã£o\n",
    "collection_name = \"meus_documentos_rag\"\n",
    "try:\n",
    "    client.delete_collection(collection_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = client.create_collection(name=collection_name)\n",
    "print(f\"âœ… ColeÃ§Ã£o '{collection_name}' criada\")\n",
    "\n",
    "# Extrair dados dos chunks\n",
    "textos_chunks = []\n",
    "ids_chunks = []\n",
    "metadados_simples = []\n",
    "\n",
    "for i, chunk in enumerate(chunks_reais):\n",
    "    # Garantir que temos texto\n",
    "    if 'text' in chunk and chunk['text'].strip():\n",
    "        textos_chunks.append(chunk['text'])\n",
    "        ids_chunks.append(f\"chunk_{i}\")  # ID simples\n",
    "        \n",
    "        # Metadados muito simples (apenas arquivo fonte)\n",
    "        if 'source_file' in chunk:\n",
    "            metadados_simples.append({'source': chunk['source_file']})\n",
    "        else:\n",
    "            metadados_simples.append({'source': 'unknown'})\n",
    "\n",
    "print(f\"\\nğŸ“Š Dados preparados:\")\n",
    "print(f\"   ğŸ“ {len(textos_chunks)} textos vÃ¡lidos\")\n",
    "print(f\"   ğŸ·ï¸ {len(ids_chunks)} IDs criados\") \n",
    "print(f\"   ğŸ“‹ {len(metadados_simples)} metadados criados\")\n",
    "\n",
    "# Verificar se temos dados vÃ¡lidos\n",
    "if len(textos_chunks) == 0:\n",
    "    print(\"âŒ Nenhum texto vÃ¡lido encontrado!\")\n",
    "    exit()\n",
    "\n",
    "# Gerar embeddings\n",
    "print(f\"\\nğŸ§  Gerando embeddings...\")\n",
    "start_time = time.time()\n",
    "embeddings_chunks = embedding_model.encode(textos_chunks)\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ… Embeddings gerados:\")\n",
    "print(f\"   â±ï¸ Tempo: {embedding_time:.2f} segundos\")\n",
    "print(f\"   ğŸ“Š Shape: {embeddings_chunks.shape}\")\n",
    "\n",
    "# Adicionar ao ChromaDB\n",
    "print(f\"\\nğŸ’¾ Adicionando ao banco vetorial...\")\n",
    "try:\n",
    "    collection.add(\n",
    "        embeddings=embeddings_chunks.tolist(),\n",
    "        documents=textos_chunks,\n",
    "        metadatas=metadados_simples,\n",
    "        ids=ids_chunks\n",
    "    )\n",
    "    \n",
    "    total_items = collection.count()\n",
    "    print(f\"âœ… Sucesso! {total_items} chunks indexados\")\n",
    "    \n",
    "    # Testar uma busca rÃ¡pida\n",
    "    print(f\"\\nğŸ§ª Teste rÃ¡pido de busca:\")\n",
    "    resultados_teste = collection.query(\n",
    "        query_texts=[\"inteligÃªncia artificial\"],\n",
    "        n_results=3\n",
    "    )\n",
    "    print(f\"   âœ… Busca funcionando: {len(resultados_teste['documents'][0])} resultados encontrados\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro ao adicionar: {e}\")\n",
    "    print(\"ğŸ’¡ Vamos tentar sem metadados...\")\n",
    "    \n",
    "    # Tentar sem metadados\n",
    "    try:\n",
    "        collection.add(\n",
    "            embeddings=embeddings_chunks.tolist(),\n",
    "            documents=textos_chunks,\n",
    "            ids=ids_chunks\n",
    "            # Sem metadatas\n",
    "        )\n",
    "        \n",
    "        total_items = collection.count()\n",
    "        print(f\"âœ… Sucesso sem metadados! {total_items} chunks indexados\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Erro mesmo sem metadados: {e2}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Banco vetorial estÃ¡ pronto!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7e19e5d-4df2-46f7-8b83-8ac938ab9e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Primeira Busca SemÃ¢ntica Real\n",
      "==================================================\n",
      "âœ… Banco vetorial disponÃ­vel: 325 chunks\n",
      "ğŸ§ª TESTE 1: ExperiÃªncia Profissional\n",
      "\n",
      "â“ Pergunta: 'experiÃªncia com inteligÃªncia artificial e machine learning'\n",
      "------------------------------------------------------------\n",
      "ğŸ” Debug - Chaves do resultado: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\n",
      "ğŸ“„ Resultado 1 (ID: chunk_323):\n",
      "   ğŸ¯ Similaridade: -0.184 (quanto maior, mais similar)\n",
      "   ğŸ“ Texto encontrado:\n",
      "      'â€¢ Explainable AI (XAI) in FER: Enhancing the interpretability of FER models to \n",
      "understand how they arrive at their predictions, which is crucial for building trust and \n",
      "addressing ethical concerns.\n",
      "â€¢ Edge AI for FER: Deploying FER models on edge devices for real-time, on-device \n",
      "processing, reducin...'\n",
      "\n",
      "ğŸ“„ Resultado 2 (ID: chunk_90):\n",
      "   ğŸ¯ Similaridade: -0.347 (quanto maior, mais similar)\n",
      "   ğŸ“ Texto encontrado:\n",
      "      'the same rules as humans. This pipeline is detailed in Appendix A3.\n",
      "Difficulty tiers. To streamline navigation, we adopt a Codeforces-style rating heuristic for difficulty\n",
      "labels. The Elo difficulty of a problem means that a contestant of the corresponding rating can\n",
      "solve the problem with a 50% pro...'\n",
      "\n",
      "ğŸ“„ Resultado 3 (ID: chunk_322):\n",
      "   ğŸ¯ Similaridade: -0.356 (quanto maior, mais similar)\n",
      "   ğŸ“ Texto encontrado:\n",
      "      'of models across diverse real-world datasets and the need for more comprehensive \n",
      "datasets that account for variations in head position and other complexities. Ethical \n",
      "considerations, including privacy, algorithmic bias, and potential misuse, are also critical \n",
      "areas that require ongoing attention ...'\n",
      "\n",
      "\n",
      "ğŸ§ª TESTE 2: CompetiÃ§Ãµes e Kaggle\n",
      "\n",
      "â“ Pergunta: 'participaÃ§Ã£o em competiÃ§Ã£o Kaggle'\n",
      "------------------------------------------------------------\n",
      "ğŸ” Debug - Chaves do resultado: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\n",
      "ğŸ“„ Resultado 1 (ID: chunk_2):\n",
      "   ğŸ¯ Similaridade: -0.103 (quanto maior, mais similar)\n",
      "   ğŸ“ Texto encontrado:\n",
      "      'No Tribunal de Contas do Estado do PR como estagiÃ¡rio pude observar projetos que poderiam ser otimizados em processos adminsitrativos, onde pude participar do primeiro chatbot alimentado com as normas internas da instituiÃ§Ã£o. \n",
      "Na FederaÃ§Ã£o do Estado do Parana pude participar na implementaÃ§Ã£o do chat...'\n",
      "\n",
      "ğŸ“„ Resultado 2 (ID: chunk_0):\n",
      "   ğŸ¯ Similaridade: -0.186 (quanto maior, mais similar)\n",
      "   ğŸ“ Texto encontrado:\n",
      "      'jÃ¡ trabalhei com Llama inclusive tenho participaÃ§Ã£o da primeira equipe de IA em uma competiÃ§Ã£o no Kaggle, trabalho diariamente com OpenAI, Gemini e Claude da Anthopric.\n",
      "Ainda nÃ£o tive oportunidade de trabalhar com LangChain porem estou trabalhando com automaÃ§Ã£o com N8N.\n",
      "JÃ¡ trabalhei com python e fas...'\n",
      "\n",
      "ğŸ“„ Resultado 3 (ID: chunk_26):\n",
      "   ğŸ¯ Similaridade: -0.197 (quanto maior, mais similar)\n",
      "   ğŸ“ Texto encontrado:\n",
      "      '// Responda com:\n",
      "// \"Claro! Um dos nossos especialistas pode te atender pelo WhatsApp. Clique abaixo para conversar com a gente direto e tirar todas as suas dÃºvidas. ğŸ‘‰ (https://wa.me/5541987249685?text=)\"\n",
      "\n",
      "// ---\n",
      "\n",
      "// ğŸ“Œ **Quando nÃ£o identificar claramente a intenÃ§Ã£o**, responda com foco em:\n",
      "// * Info...'\n",
      "\n",
      "\n",
      "ğŸ§ª TESTE 3: CÃ³digo e ProgramaÃ§Ã£o\n",
      "\n",
      "â“ Pergunta: 'cÃ³digo Python machine learning'\n",
      "------------------------------------------------------------\n",
      "ğŸ” Debug - Chaves do resultado: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\n",
      "ğŸ“„ Resultado 1 (ID: chunk_317):\n",
      "   ğŸ¯ Similaridade: -0.143 (quanto maior, mais similar)\n",
      "   ğŸ“ Texto encontrado:\n",
      "      'address speciï¬c challenges.\n",
      "Traditional datasets like CK+ (Extended Cohn-Kanade Dataset), JAFFE (Japanese Female \n",
      "Facial Expression), and FER-2013 remain important benchmarks. However, research \n",
      "indicates that models trained on these datasets may face limitations when dealing with \n",
      "variations in hea...'\n",
      "\n",
      "ğŸ“„ Resultado 2 (ID: chunk_131):\n",
      "   ğŸ¯ Similaridade: -0.156 (quanto maior, mais similar)\n",
      "   ğŸ“ Texto encontrado:\n",
      "      'LiveCodeBench Pro\n",
      "[52] Maxim Shipko. New features: friends, tags and more, 2011. URL https://codeforces.com/\n",
      "blog/entry/1679.\n",
      "[53] Jiaxin Wen, Ruiqi Zhong, Pei Ke, Zhihong Shao, Hongning Wang, and Minlie Huang. Learning\n",
      "task decomposition to assist humans in competitive programming. In Proceedings o...'\n",
      "\n",
      "ğŸ“„ Resultado 3 (ID: chunk_76):\n",
      "   ğŸ¯ Similaridade: -0.208 (quanto maior, mais similar)\n",
      "   ğŸ“ Texto encontrado:\n",
      "      'LiveCodeBench Pro\n",
      "Abstract. Recent reports claim that large language models (LLMs) now outperform elite humans\n",
      "in competitive programming. Drawing on knowledge from a group of medalists in international\n",
      "algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and\n",
      "whe...'\n",
      "\n",
      "\n",
      "ğŸ§ª TESTE 4: Ferramentas de IA\n",
      "\n",
      "â“ Pergunta: 'OpenAI Gemini Claude LLM'\n",
      "------------------------------------------------------------\n",
      "ğŸ” Debug - Chaves do resultado: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\n",
      "ğŸ“„ Resultado 1 (ID: chunk_138):\n",
      "   ğŸ¯ Similaridade: -0.207 (quanto maior, mais similar)\n",
      "   ğŸ“ Texto encontrado:\n",
      "      'Full Model Name Org. Lic. Cut-off Date\n",
      "Reasoning Models\n",
      "o4-mini-high (250416) [47] OpenAI Proprietary May 31 2024\n",
      "Gemini 2.5 Pro Experimental 03-25 [22] Google Proprietary January 2025\n",
      "o3-mini (250131) [46] OpenAI Proprietary Sep 30 2023\n",
      "DeepSeek R1 [25] DeepSeek MIT Not specified\n",
      "Gemini 2.5 Flash P...'\n",
      "\n",
      "ğŸ“„ Resultado 2 (ID: chunk_126):\n",
      "   ğŸ¯ Similaridade: -0.288 (quanto maior, mais similar)\n",
      "   ğŸ“ Texto encontrado:\n",
      "      '[34] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\n",
      "Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint\n",
      "arXiv:2412.16720, 2024.\n",
      "14...'\n",
      "\n",
      "ğŸ“„ Resultado 3 (ID: chunk_123):\n",
      "   ğŸ¯ Similaridade: -0.295 (quanto maior, mais similar)\n",
      "   ğŸ“ Texto encontrado:\n",
      "      'LiveCodeBench Pro\n",
      "[20] Google DeepMind. Model card: Gemini 2.0 flash reasoning.https://blog.google/technology/\n",
      "google-deepmind/gemini-model-updates-february-2025/ , 2025.\n",
      "[21] Google DeepMind. Model card: Gemini 2.5 flash. https://blog.google/products/gemini/\n",
      "gemini-2-5-flash-preview/ , 2025.\n",
      "[22] G...'\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ANÃLISE DOS RESULTADOS:\n",
      "   ExperiÃªncia IA: -0.184\n",
      "   Kaggle: -0.103\n",
      "   CÃ³digo Python: -0.143\n",
      "   Ferramentas IA: -0.207\n",
      "\n",
      "ğŸ† Melhor match encontrado: -0.103\n",
      "   ğŸ” O sistema encontrou algo, mas pode nÃ£o ser muito especÃ­fico\n",
      "\n",
      "âœ¨ Buscas semÃ¢nticas concluÃ­das!\n"
     ]
    }
   ],
   "source": [
    "# CÃ©lula 10: Buscas semÃ¢nticas - VERSÃƒO CORRIGIDA\n",
    "print(\"ğŸ” Primeira Busca SemÃ¢ntica Real\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Verificar se temos o banco vetorial\n",
    "try:\n",
    "    total_items = collection.count()\n",
    "    print(f\"âœ… Banco vetorial disponÃ­vel: {total_items} chunks\")\n",
    "except:\n",
    "    print(\"âŒ Banco vetorial nÃ£o encontrado!\")\n",
    "    exit()\n",
    "\n",
    "# FunÃ§Ã£o corrigida para fazer buscas\n",
    "def buscar_documentos(pergunta, num_resultados=3):\n",
    "    print(f\"\\nâ“ Pergunta: '{pergunta}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Fazer a busca\n",
    "        resultados = collection.query(\n",
    "            query_texts=[pergunta],\n",
    "            n_results=num_resultados\n",
    "        )\n",
    "        \n",
    "        # Debug: mostrar estrutura do resultado\n",
    "        print(f\"ğŸ” Debug - Chaves do resultado: {list(resultados.keys())}\")\n",
    "        \n",
    "        # Extrair informaÃ§Ãµes de forma segura\n",
    "        documentos_encontrados = resultados.get('documents', [[]])[0]\n",
    "        ids = resultados.get('ids', [[]])[0]\n",
    "        \n",
    "        # ChromaDB pode retornar 'distances' ou nÃ£o, vamos verificar\n",
    "        if 'distances' in resultados:\n",
    "            distancias = resultados['distances'][0]\n",
    "        else:\n",
    "            # Se nÃ£o tem distances, simular com valores neutros\n",
    "            distancias = [0.5] * len(documentos_encontrados)\n",
    "            print(\"âš ï¸ Distances nÃ£o disponÃ­veis, usando valores simulados\")\n",
    "        \n",
    "        # Verificar se temos resultados\n",
    "        if not documentos_encontrados:\n",
    "            print(\"âŒ Nenhum resultado encontrado!\")\n",
    "            return [], []\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        for i, (doc, distancia, id_chunk) in enumerate(zip(documentos_encontrados, distancias, ids)):\n",
    "            similaridade = 1 - distancia  # Converter distÃ¢ncia em similaridade\n",
    "            print(f\"ğŸ“„ Resultado {i+1} (ID: {id_chunk}):\")\n",
    "            print(f\"   ğŸ¯ Similaridade: {similaridade:.3f} (quanto maior, mais similar)\")\n",
    "            print(f\"   ğŸ“ Texto encontrado:\")\n",
    "            print(f\"      '{doc[:300]}...'\")\n",
    "            print()\n",
    "        \n",
    "        return documentos_encontrados, distancias\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro na busca: {e}\")\n",
    "        return [], []\n",
    "\n",
    "# Teste 1: Buscar informaÃ§Ãµes sobre experiÃªncia profissional\n",
    "print(\"ğŸ§ª TESTE 1: ExperiÃªncia Profissional\")\n",
    "docs1, dist1 = buscar_documentos(\"experiÃªncia com inteligÃªncia artificial e machine learning\")\n",
    "\n",
    "# Teste 2: Buscar sobre Kaggle (mencionado no seu background)\n",
    "print(\"\\nğŸ§ª TESTE 2: CompetiÃ§Ãµes e Kaggle\") \n",
    "docs2, dist2 = buscar_documentos(\"participaÃ§Ã£o em competiÃ§Ã£o Kaggle\")\n",
    "\n",
    "# Teste 3: Buscar sobre cÃ³digo/programaÃ§Ã£o\n",
    "print(\"\\nğŸ§ª TESTE 3: CÃ³digo e ProgramaÃ§Ã£o\")\n",
    "docs3, dist3 = buscar_documentos(\"cÃ³digo Python machine learning\")\n",
    "\n",
    "# Teste 4: Buscar sobre ferramentas de IA\n",
    "print(\"\\nğŸ§ª TESTE 4: Ferramentas de IA\")\n",
    "docs4, dist4 = buscar_documentos(\"OpenAI Gemini Claude LLM\")\n",
    "\n",
    "# AnÃ¡lise de resultados (sÃ³ se temos dados vÃ¡lidos)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ANÃLISE DOS RESULTADOS:\")\n",
    "\n",
    "todos_testes = [\n",
    "    (\"ExperiÃªncia IA\", dist1),\n",
    "    (\"Kaggle\", dist2), \n",
    "    (\"CÃ³digo Python\", dist3),\n",
    "    (\"Ferramentas IA\", dist4)\n",
    "]\n",
    "\n",
    "similaridades_validas = []\n",
    "for nome, distancias in todos_testes:\n",
    "    if distancias:  # Se tem resultados\n",
    "        melhor_similaridade = 1 - min(distancias)\n",
    "        similaridades_validas.append(melhor_similaridade)\n",
    "        print(f\"   {nome}: {melhor_similaridade:.3f}\")\n",
    "    else:\n",
    "        print(f\"   {nome}: Sem resultados\")\n",
    "\n",
    "if similaridades_validas:\n",
    "    melhor_busca = max(similaridades_validas)\n",
    "    print(f\"\\nğŸ† Melhor match encontrado: {melhor_busca:.3f}\")\n",
    "    \n",
    "    if melhor_busca > 0.8:\n",
    "        print(\"   ğŸ’š Excelente! O sistema entendeu muito bem a pergunta\")\n",
    "    elif melhor_busca > 0.6:\n",
    "        print(\"   ğŸ’› Bom! O sistema encontrou conteÃºdo relevante\")\n",
    "    else:\n",
    "        print(\"   ğŸ” O sistema encontrou algo, mas pode nÃ£o ser muito especÃ­fico\")\n",
    "else:\n",
    "    print(\"âš ï¸ Nenhum resultado vÃ¡lido encontrado\")\n",
    "\n",
    "print(\"\\nâœ¨ Buscas semÃ¢nticas concluÃ­das!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fab857-021f-460d-a493-d632a51f840d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain RAG",
   "language": "python",
   "name": "langchain-rag-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
